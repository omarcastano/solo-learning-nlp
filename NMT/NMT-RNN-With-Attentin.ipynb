{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c08a40b",
   "metadata": {},
   "source": [
    "## Neural Machine Traslation using Encoder-Decoder Architecture\n",
    "\n",
    "The aim of this notebook is to implement a Neural Machine Traslation (NMT) using basic [encoder-decoder](https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf) approach with [Bahandanau attention mechanism](https://arxiv.org/pdf/1409.0473.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23eb3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#!mkdir MNT-Dataset\n",
    "#!wget -P MNT-Dataset/ https://www.manythings.org/anki/spa-eng.zip\n",
    "#!unzip MNT-Dataset/spa-eng.zip -d MNT-Dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842c663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaries\n",
    "import torch\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import multiprocessing as mp\n",
    "\n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchmetrics.functional import bleu_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d9132c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139705, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go.</td>\n",
       "      <td>ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go.</td>\n",
       "      <td>vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>go.</td>\n",
       "      <td>vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>go.</td>\n",
       "      <td>váyase.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hi.</td>\n",
       "      <td>hola.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english  spanish\n",
       "0     go.      ve.\n",
       "1     go.    vete.\n",
       "2     go.    vaya.\n",
       "3     go.  váyase.\n",
       "4     hi.    hola."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = pd.read_table(\"MNT-Dataset/spa.txt\", header=None, names=[\"english\", \"spanish\", \"ref\"]).drop(labels=[\"ref\"], axis=1)\n",
    "print(dataset.shape)\n",
    "dataset.english = dataset.english.str.lower()\n",
    "dataset.spanish = dataset.spanish.str.lower()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa71a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a tokenizer using spacy\n",
    "class Tokenizer:\n",
    "    def __init__(self, language: str = None) -> None:\n",
    "        \"\"\"\n",
    "        A simple tokenizer class that uses Spacy to tokenize text.\n",
    "\n",
    "        Parameteres:\n",
    "        ------------\n",
    "            language (str, optional): The language of the text to be tokenized. Defaults to None.\n",
    "                Supported languages are 'sp' for Spanish and 'en' for English.\n",
    "        \"\"\"\n",
    "\n",
    "        if language == \"sp\":\n",
    "            self.nlp = spacy.load(\"es_core_news_sm\")  # load the Spanish Spacy model\n",
    "        elif language == \"en\":\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")  # load the English Spacy model\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Tokenizes a given text using the Spacy tokenizer.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            A list of strings representing the tokens in the text.\n",
    "        \"\"\"\n",
    "\n",
    "        return [w.text for w in self.nlp.tokenizer(text)]  # return the text tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74cb670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we a language class that represents a language and its vocabulary\n",
    "class Lang:\n",
    "    def __init__(self, name:str, language:str=\"sp\"):\n",
    "        \"\"\"\n",
    "        A class for language preprocessing and encoding. It uses a tokenizer to split text into tokens, and encodes\n",
    "        these tokens into integer values. It also provides methods to add sentences and words to the vocabulary, and to\n",
    "        transform text into its encoded form.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        name : str\n",
    "            A name for the language object.\n",
    "        language : str, default='sp'\n",
    "            The language of the text to process. Currently supported languages are 'sp' (Spanish) and 'en' (English).\n",
    "        \"\"\"\n",
    "        \n",
    "        self.name = name\n",
    "        self.language = language\n",
    "        self.word2index = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "        self.tokenizer = Tokenizer(language)\n",
    "\n",
    "    def addSentence(self, sentence:str):\n",
    "        \"\"\"\n",
    "        Add a sentence to the vocabulary.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        sentence : str\n",
    "            The sentence to add.\n",
    "        \"\"\"\n",
    "        \n",
    "        for word in self.tokenizer(sentence):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word:str):\n",
    "        \"\"\"\n",
    "        Add a word to the vocabulary.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        word : str\n",
    "            The word to add.\n",
    "        \"\"\"\n",
    "        \n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def fit(self, dataset:List[str]):\n",
    "        \"\"\"\n",
    "        Build the vocabulary from a dataset.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataset : list\n",
    "            A list of sentences to add to the vocabulary.\n",
    "        \"\"\"\n",
    "        \n",
    "        for data in tqdm(dataset):\n",
    "            self.addSentence(data)\n",
    "\n",
    "    def transform(self, text:str, padding:bool=True):\n",
    "        \"\"\"\n",
    "        Transform text into its encoded form.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The text to encode.\n",
    "        padding : bool, default=True\n",
    "            Whether to pad the sequence to the maximum sequence length.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        encoding : list\n",
    "            A list of integers representing the encoded sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = self.tokenizer(text)\n",
    "        if padding:\n",
    "            tokens = [\"<start>\"] + tokens + [\"<end>\"]\n",
    "            tokens = tokens\n",
    "\n",
    "        encoding = [self.word2index[tk] if tk in self.word2index.keys() else 3 for tk in tokens]\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def inverse_transform(self, tokens:List):\n",
    "        \"\"\"\n",
    "        Decodes the encoded sequence of integers using the vocabulary of the language.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "            tokens: list\n",
    "                The encoded sequence of integers to decode.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            str: The decoded sentence.\n",
    "        \"\"\"\n",
    "        \n",
    "        words = [self.index2word[tk] for tk in tokens]\n",
    "\n",
    "        return \" \".join(words)\n",
    "    \n",
    "    @staticmethod\n",
    "    def right_padding_per_batch(batch: tuple):\n",
    "        \"\"\"\n",
    "        Pads the sequence of tokens with 0s to match the sequence length per batch. \n",
    "        This method will be pass to the collate_fn argument of the Dataloader class.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "            batch: tuple \n",
    "                The sequence of tokens to pad.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            tuple: The padded sequence of tokens in the batch.\n",
    "        \"\"\"\n",
    "    \n",
    "        en_text_bs, sp_text_bs = [], []\n",
    "\n",
    "        for en_text, sp_text in batch:\n",
    "            en_text_bs.append(en_text)\n",
    "            sp_text_bs.append(sp_text)\n",
    "\n",
    "        en_text_bs = pad_sequence(en_text_bs, padding_value=0)\n",
    "        sp_text_bs = pad_sequence(sp_text_bs, padding_value=0)\n",
    "\n",
    "        return en_text_bs, sp_text_bs\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd227107",
   "metadata": {},
   "source": [
    "# Data loader\n",
    "\n",
    "We define a custom data loader that output the token for the sentences in spanish and english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abbf6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset:pd.DataFrame, sp_lang:Lang=None, en_lang:Lang=None):\n",
    "        \"\"\"\n",
    "        A PyTorch custom dataset for language translation.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        dataset : pandas DataFrame\n",
    "            The dataset containing the English and Spanish sentences.\n",
    "        sp_lang: Lang\n",
    "            The language object for the Spanish language. Default None\n",
    "        en_lang: Lang\n",
    "            The language object for the English language. Default None\n",
    "        \"\"\"\n",
    "    \n",
    "        self.dataset = dataset\n",
    "\n",
    "        if isinstance(sp_lang, Lang) and isinstance(en_lang, Lang):\n",
    "            self.sp_lang = sp_lang\n",
    "            self.en_lang = en_lang\n",
    "            \n",
    "        else:\n",
    "            # Initialize language objects for Spanish and English\n",
    "            self.sp_lang = Lang(\"sp\", language=\"sp\")\n",
    "            self.sp_lang.fit(dataset.spanish)\n",
    "\n",
    "            self.en_lang = Lang(\"en\", language=\"en\")\n",
    "            self.en_lang.fit(dataset.english)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        int\n",
    "            The number of samples in the dataset\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a sample from the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        idx : int\n",
    "            The index of the sample to return.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        tuple of torch.Tensor\n",
    "            The English sentence and the Spanish sentence as tensors.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the Spanish and English sentences from the dataset\n",
    "        sp_text = self.dataset.spanish.tolist()[idx]\n",
    "        en_text = self.dataset.english.tolist()[idx]\n",
    "\n",
    "        # Transform the Spanish and English sentences using the language objects\n",
    "        sp_text = self.sp_lang.transform(sp_text)\n",
    "        en_text = self.en_lang.transform(en_text)\n",
    "\n",
    "        # Convert the transformed sentences to tensors\n",
    "        sp_text = torch.Tensor(sp_text).long()\n",
    "        en_text = torch.Tensor(en_text).long()\n",
    "\n",
    "        return en_text, sp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a496826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa80123a1c934df786eda56c94b7ae57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db34b1cdb4544e85abd124fc887bc710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the dataloader\n",
    "ds_train = CustomDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99141af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the spanish and English vocab size\n",
    "sp_vocab = ds_train.sp_lang.n_words\n",
    "en_vocab = ds_train.en_lang.n_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0673072",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e703978",
   "metadata": {},
   "source": [
    "The encoder ($e$) used here is similar to the encoder implemented in NMT-RRN-NO-Attention, but in this case, as proposed in the original paper that introduced attention mechanism, we use a bidirectional GRU. The decoder is not a bidirectional RNN, so we cannot directly pass the `hidden` state of the encoder to the decoder because the shapes of the expected input hidden state of the decoder, and the output hidden state of the encoder will not match. In this direction, we use a linear layer to reduce the dimension of the encoder hidden state such that it fits with the expected dimension of the decoder hidden state. \n",
    "\n",
    "\n",
    "First, we compute the hidden states and outputs\n",
    "\n",
    "$$\\overrightarrow{O_t}, \\overrightarrow{h_t} = GRU(x_t, \\overrightarrow{h_{t-1}})$$\n",
    "$$\\overleftarrow{O_t}, \\overleftarrow{h_t} = GRU(x_t, \\overleftarrow{h_{t-1}})$$\n",
    "\n",
    "Then, we use a linear layer '$a$' to reduce the dimension of the encoder hidden state\n",
    "\n",
    "$$h_t = a([\\overrightarrow{h_t}, \\overleftarrow{h_t}])$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7f83fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_dim=100, n_layers=2, hidden_dim=10, dropout=0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Encoder class\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size: int\n",
    "            The size of the input vocabulary\n",
    "        embedding_dim: int\n",
    "            The size of the embeddings\n",
    "        n_layers: int\n",
    "            The number of layers in the encoder\n",
    "        hidden_dim: int\n",
    "            The size of the hidden dimension\n",
    "        dropout: float\n",
    "            The dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding layer\n",
    "        # input shape: either [batch_size, seq_len] or [seq_len, batch_size]\n",
    "        self.embeddings = nn.Embedding(input_size, embedding_dim)\n",
    "\n",
    "        # GRU layers\n",
    "        # input shape: [batch_size, seq_len, features] if batch_first=True\n",
    "        # input shape: [seq_len, batch_size, features] if batch_first=False\n",
    "        self.rnn = nn.GRU(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            n_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=False,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Linear layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: torch.Tensor\n",
    "            The input to the encoder\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        x: torch.Tensor\n",
    "            The output of the encoder\n",
    "        hidden: torch.Tensor\n",
    "            The hidden state of the encoder\n",
    "\n",
    "        Notes:\n",
    "        -----------\n",
    "        x shape: [seq_len, batch_size]\n",
    "        hidden shape: = [n_layers * n_directions, batch_size, hidden_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        # x shape: [seq_len, batch_size]\n",
    "\n",
    "        # Embedding Layer\n",
    "        # output shape: [seq_len, batch_size, emb_dim]\n",
    "        x = self.dropout(self.embeddings(x))\n",
    "\n",
    "        # GRU Layer\n",
    "        # output(x) shape: [seq_len, batch_size, num_directions * hidden_dim]\n",
    "        # output(hidden) shape: = [n_layers * n_directions, batch_size, hidden_dim]\n",
    "        x, hidden = self.rnn(x)\n",
    "\n",
    "        # hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        # outputs are always from the last layer\n",
    "        # output(hidden) shape: = [n_layers, batch_size, hidden_dim]\n",
    "        hidden = torch.cat((hidden[::2, :, :], hidden[1::2, :, :]), dim=-1)\n",
    "        hidden = torch.tanh(self.fc(hidden))\n",
    "\n",
    "        return x, hidden\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acbdd669",
   "metadata": {},
   "source": [
    "### Test Enconder\n",
    "\n",
    "We test that the encoder works properly using some dummy inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab72eb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 20])\n",
      "torch.Size([2, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test encoder\n",
    "encoder = Encoder(100)\n",
    "\n",
    "x = torch.tensor([[1, 3, 4], [4, 5, 6]], dtype=torch.long)\n",
    "x, hidden = encoder(x.T)\n",
    "print(x.shape)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2842730d",
   "metadata": {},
   "source": [
    "# Attetion Mechanism"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd65b8a7",
   "metadata": {},
   "source": [
    "In this implementation we have used the [Bahandanau attention mechanism](https://arxiv.org/pdf/1409.0473.pdf), which was the very first attention mechanism published to solve the neural machine translation problem. The attention mechanism is introduced as part of the decoder. More specifically, the hidden state for the decoder is written as:\n",
    "\n",
    "$$s_i = f(s_{i-1}, y_{i-1}, c_i)$$\n",
    "\n",
    "Different from the [S2S NMT](https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf), now the context vector $C_i$ depends on a sequence of hidden states $(h_1, h_2, ... ,h_{T_x})$ to which an encoder maps the input sentence. The context vector $C_i$ is then computed as a weighted sum of these hidden states\n",
    "\n",
    "$$C_i = \\sum_{j=1}^{T_x} \\alpha_{ij}h_j$$\n",
    "\n",
    "The weights $\\alpha_{ij}$ fot each $h_j$ is computed by\n",
    "\n",
    "$$\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}$$\n",
    "\n",
    "with $$e_{ij} = a(s_{i-1}, h_j)$$\n",
    "\n",
    "where $a$ denotes is a multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a6a88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int) -> None:\n",
    "        \"\"\"\n",
    "        Attention Mechanism\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_dim : int\n",
    "            hidden dimension of the RNN\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Linear layers\n",
    "        self.attn = nn.Linear(self.hidden_dim * 3, self.hidden_dim)\n",
    "        self.v = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_output):\n",
    "        # decoder_hidden shape: [n_layers * n_directions, batch_size, hidden_dim]\n",
    "        # encoder_output shape: [seq_len, batch_size, hidden_dim * 2]\n",
    "\n",
    "        seq_len = encoder_output.shape[0]\n",
    "\n",
    "        # decoder shape: [1, batch_size, hidden_dim]\n",
    "        decoder_hidden = decoder_hidden[[-1], :, :].repeat(seq_len, 1, 1)\n",
    "\n",
    "        # e shape: [seq_len, batch_size, hidden_dim]\n",
    "        e = torch.tanh(self.attn(torch.cat((decoder_hidden, encoder_output), dim=-1)))\n",
    "\n",
    "        # e shape: [seq_len, batch_size, 1]\n",
    "        e = self.v(e)\n",
    "\n",
    "        # alpha shape: [seq_len, batch_size, 1]\n",
    "        alpha = torch.softmax(e, dim=0)\n",
    "\n",
    "        # context shape: [batch_size, 1, hidden_dim * 2]\n",
    "        context = torch.sum(alpha * encoder_output, dim=0).unsqueeze(0)\n",
    "\n",
    "        return context\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2bab6c8",
   "metadata": {},
   "source": [
    "### Test the attention mechanism\n",
    "\n",
    "Now we test the attention mechanism using some dummy inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79ef5beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 20])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test Attention\n",
    "bs = 2\n",
    "hidden_dim = 10\n",
    "seq_len = 3\n",
    "n_layers = 2\n",
    "\n",
    "decoder_hidden = torch.rand((n_layers, bs, hidden_dim))\n",
    "encoder_output = torch.rand((seq_len, bs, hidden_dim * 2))\n",
    "\n",
    "att = Attention(hidden_dim)\n",
    "context = att(decoder_hidden, encoder_output)\n",
    "context.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58f3011b",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "The decoder make use of the attention layer defined above, which takes previous decoder hidden states $s_{i-1}$, all of the encoder hidden states $\\{h_1, h_2, .... , h_{T_x} \\}$, and returns the context vector $C_t$. To compute the decoder hidden state at each time $s_t$, we use the following equation:\n",
    "\n",
    "$$s_t = d(y_{t-1}, s_{t-1}, c)$$\n",
    "\n",
    "where $y_{t-1}$ is the previous predicted token, $s_{t-1}$ is the previous hidden stat and $C_t$ is the context vector computed using the attention layer.\n",
    "\n",
    "We then pass the output of the decoder to a linear layer $f$, to make prediction of the next word in the target sentence $\\hat y_{t+1}$:\n",
    "\n",
    "$$\\hat y_{t+1} = f(s_{t})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "926ecf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        embedding_dim=100,\n",
    "        n_layers=2,\n",
    "        hidden_dim=10,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A class representing a decoder in a sequence-to-sequence model.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size: int\n",
    "            The number of input tokens.\n",
    "        output_size: int\n",
    "            The number of output tokens.\n",
    "        embedding_dim: int\n",
    "            The dimension of the embedding layer.\n",
    "        n_layers: int\n",
    "            The number of layers in the RNN.\n",
    "        hidden_dim: int\n",
    "            The number of hidden units in the RNN.\n",
    "        dropout: float\n",
    "            The dropout rate. \n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding layer\n",
    "        # input shape: either [batch_size, seq_len] or [seq_len, batch_size]\n",
    "        self.embeddings = nn.Embedding(input_size, embedding_dim)\n",
    "\n",
    "        # GRU layers\n",
    "        # input shape: [batch_size, seq_len, features] if batch_first=True\n",
    "        # input shape: [seq_len, batch_size, features] if batch_first=False\n",
    "        self.rnn = nn.GRU(\n",
    "            embedding_dim + hidden_dim * 2,\n",
    "            hidden_dim,\n",
    "            n_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # Fully confected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Attention\n",
    "        self.attn = Attention(self.hidden_dim)\n",
    "\n",
    "    def forward(self, x, decoder_hidden, encoder_output):\n",
    "        \"\"\"\n",
    "        Perform forward pass of the decoder.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: torch.Tensor\n",
    "            The input tensor with shape [seq_len=1, batch_size].\n",
    "        decoder_hidden: torch.Tensor\n",
    "            The hidden state of the decoder.\n",
    "        encoder_output: torch.Tensor\n",
    "            The output of the encoder.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        x: torch.Tensor\n",
    "            The output of the decoder.\n",
    "        hidden: torch.Tensor\n",
    "            The hidden state of the decoder.\n",
    "        context: torch.Tensor\n",
    "            The attention context.\n",
    "        \"\"\"    \n",
    "\n",
    "\n",
    "        # x shape: [seq_len=1, batch_size]\n",
    "        # hidden shape: = [n_layers * n_directions, batch_size, hid_dim]\n",
    "        # context shape: [1, batch_size, hid_dim]\n",
    "\n",
    "        # Embeddings\n",
    "        # x shape: [seq_len=1, batch_size, emb_dim]\n",
    "        x = self.dropout(self.embeddings(x))\n",
    "\n",
    "        # Attention mechanism\n",
    "        context = self.attn(decoder_hidden, encoder_output)\n",
    "\n",
    "        # Concatenation\n",
    "        # x shape: [seq_len, batch_size, emb_dim + hid_dim]\n",
    "        x = torch.cat((x, context), dim=2)\n",
    "\n",
    "        # GRU Layer\n",
    "        # x shape: [seq_len=1, batch_size, num_directions * hidden_dim]\n",
    "        # hidden shape: = [n_layers * n_directions, batch_size, hidden_dim]\n",
    "        x, hidden = self.rnn(x, decoder_hidden)\n",
    "\n",
    "        # Fully connected layer\n",
    "        # x shape: [batch_size, output_size]\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, hidden\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b43b64e6",
   "metadata": {},
   "source": [
    "### Test the decoder\n",
    "\n",
    "Now we test that the decoder is working as we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "338592b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 100])\n",
      "torch.Size([2, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "# test Decoder\n",
    "bs = 2\n",
    "hidden_dim = 10\n",
    "seq_len = 3\n",
    "n_layers = 2\n",
    "\n",
    "decoder_hidden = torch.rand((n_layers, bs, hidden_dim))\n",
    "encoder_output = torch.rand((seq_len, bs, hidden_dim * 2))\n",
    "x = torch.tensor([[1], [4]], dtype=torch.long)\n",
    "\n",
    "decoder = Decoder(100, output_size=100)\n",
    "x, hidden = decoder(x.T, decoder_hidden, encoder_output)\n",
    "print(x.shape)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "851ff59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        en_vocab,\n",
    "        sp_vocab,\n",
    "        en_lang,\n",
    "        sp_lang,\n",
    "        embedding_dim,\n",
    "        n_layers=2,\n",
    "        hidden_dim=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Neural Machine Translation model (NMT) based on encoder-decoder architecture without attention.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        en_vocab : int\n",
    "            Size of the English vocabulary.\n",
    "        sp_vocab : int\n",
    "            Size of the Spanish vocabulary.\n",
    "        en_lang : object\n",
    "            English language object.\n",
    "        sp_lang : object\n",
    "            Spanish language object.\n",
    "        embedding_dim : int\n",
    "            Dimension of the word embedding space.\n",
    "        n_layers : int, optional\n",
    "            Number of layers in the encoder and decoder (default is 2).\n",
    "        hidden_dim : int, optional\n",
    "            Dimension of the hidden state in the encoder and decoder (default is 10).\n",
    "        \"\"\"\n",
    "\n",
    "        self.en_vocab = en_vocab\n",
    "        self.sp_vocab = sp_vocab\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sp_lang = sp_lang\n",
    "        self.en_lang = en_lang\n",
    "\n",
    "        # Define Encoder and Decoder\n",
    "        self.encoder = Encoder(en_vocab, embedding_dim, n_layers, hidden_dim)\n",
    "        self.decoder = Decoder(sp_vocab, sp_vocab, embedding_dim, n_layers, hidden_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Forward pass of the NMT model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : tensor\n",
    "            Tensor of shape (seq_len, batch_size) containing the input sequences in English.\n",
    "        y : tensor\n",
    "            Tensor of shape (seq_len, batch_size) containing the input sequences in Spanish.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : tensor\n",
    "            Tensor of shape (seq_len, batch_size, sp_vocab) containing the predicted Spanish sequences.\n",
    "        \"\"\"\n",
    "\n",
    "        # shape x: [seq_len, batch_size]\n",
    "        # shape y: [seq_len, batch_size]\n",
    "\n",
    "        target_len = y.shape[0]\n",
    "        batch_size = x.shape[1]\n",
    "\n",
    "        # outputs tensor\n",
    "        # outputs shape: [seq_len, batch_size, vocab_size]\n",
    "        outputs = torch.zeros(target_len, batch_size, self.sp_vocab).to(self.device)\n",
    "\n",
    "        # Encoder\n",
    "        # y_encoder shape: [seq_len, batch_size, num_directions * hidden_dim]\n",
    "        # hidden shape: = [n_layers * n_directions, batch_size, hidden_dim]\n",
    "        y_encoder, hidden = self.encoder(x)\n",
    "\n",
    "        # Initial prediction\n",
    "        # x_decoder shape: [1, batch_size]\n",
    "        x_decoder = y[[0], :]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # output(output) shape: [batch_size, output_size]\n",
    "            # output(hidden) shape: = [n_layers * n_directions, batch_size, hidden_dim]\n",
    "            output, hidden = self.decoder(x_decoder, hidden, y_encoder)\n",
    "            outputs[[t], :, :] = output\n",
    "            y_decoder = output.argmax(-1)\n",
    "            x_decoder = y[[t], :] if np.random.random() < 0.5 else y_decoder\n",
    "\n",
    "        # output shape: [seq_len, batch_size, vocab_size]\n",
    "        return outputs\n",
    "\n",
    "    def translate_sentence(self, x):\n",
    "        \"\"\"\n",
    "        Translate an English sentence to Spanish.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : str\n",
    "            English sentence to translate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : list\n",
    "            List of integers representing the predicted Spanish sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        # Transform input text to tokens\n",
    "        x = (\n",
    "            torch.Tensor(self.en_lang.transform(x))\n",
    "            .long()\n",
    "            .reshape(-1, 1)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        # define output array\n",
    "        outputs = []\n",
    "\n",
    "        # Initial token <start>\n",
    "        x_decoder = torch.Tensor([[1]]).long().to(self.device)\n",
    "\n",
    "        # pass sentence to the encoder\n",
    "        y_encoder, hidden = self.encoder(x)\n",
    "\n",
    "        t = 1\n",
    "\n",
    "        # this will run until prediction is <end> or t >= 200\n",
    "        while x_decoder != 2:\n",
    "            output, hidden = self.decoder(x_decoder, hidden, y_encoder)\n",
    "            outputs.append(output.argmax(-1).item())\n",
    "            x_decoder = y_decoder = output.argmax(-1)\n",
    "\n",
    "            if t >= 200:\n",
    "                break\n",
    "\n",
    "            t += 1\n",
    "\n",
    "        return self.sp_lang.inverse_transform(outputs)\n",
    "\n",
    "    def config_model(self, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Configure the NMT model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        device : str, optional\n",
    "            Device to use (default is \"cuda\").\n",
    "        \"\"\"\n",
    "\n",
    "        # define device to operate\n",
    "        self.device = device\n",
    "\n",
    "        # set model's device\n",
    "        self.to(self.device)\n",
    "\n",
    "        # define loss function\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "        # define optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Train the NMT model for one epoch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_loader : DataLoader\n",
    "            DataLoader object containing the training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logs : dict\n",
    "            Dictionary containing the training loss and BLEU score.\n",
    "        \"\"\"\n",
    "\n",
    "        running_loss = 0\n",
    "        bleu = 0\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        bar = tqdm(train_loader, leave=True)\n",
    "\n",
    "        for step, (x, y) in enumerate(bar, 1):\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # set device\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "            # forward pass\n",
    "            logits = self(x, y)  # shape: [seq_len, batch_size, vocab_size]\n",
    "\n",
    "            # Remove <start> from target\n",
    "            y = y[1:]\n",
    "            logits = logits[1:]\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.loss(logits.reshape(-1, logits.shape[2]), y.reshape(-1))\n",
    "\n",
    "            # Clip the gradient value is it exceeds > 1\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1)\n",
    "\n",
    "            # Compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weigths\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # compute running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # predictions\n",
    "            y_pred = logits.argmax(-1).detach().cpu().numpy()\n",
    "            y_pred = [\n",
    "                self.sp_lang.inverse_transform(y_pred[:, i]) for i in range(x.shape[1])\n",
    "            ]\n",
    "\n",
    "            # true labels\n",
    "            y = y.detach().cpu().numpy()\n",
    "            y = [self.sp_lang.inverse_transform(y[:, i]) for i in range(x.shape[1])]\n",
    "\n",
    "            bleu += bleu_score(y_pred, y).item()\n",
    "\n",
    "            bar.set_description(\n",
    "                f\"Train loss {round(running_loss/step, 3)}, \"\n",
    "                f\"Train BLEU {round(bleu/step, 3)}\"\n",
    "            )\n",
    "\n",
    "        logs = {\n",
    "            \"Train loss\": round(running_loss / step, 3),\n",
    "            \"Train BLEU\": round(bleu / step, 3),\n",
    "        }\n",
    "\n",
    "        return logs\n",
    "\n",
    "    def test_one_epoch(self, test_loader):\n",
    "        \"\"\"\n",
    "        Test the NMT model for one epoch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_loader : DataLoader\n",
    "            DataLoader object containing the test data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logs : dict\n",
    "            Dictionary containing the test loss and BLEU score.\n",
    "        \"\"\"\n",
    "\n",
    "        running_loss = 0\n",
    "        bleu = 0\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bar = tqdm(test_loader, leave=True)\n",
    "\n",
    "            for step, (x, y) in enumerate(bar, 1):\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # set device\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                # forward pass\n",
    "                logits = self(x, y)  # shape: [seq_len, batch_size, vocab_size]\n",
    "\n",
    "                # Remove <start> from target\n",
    "                y = y[1:]\n",
    "                logits = logits[1:]\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.loss(logits.reshape(-1, logits.shape[2]), y.reshape(-1))\n",
    "\n",
    "                # compute running loss\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # predictions\n",
    "                y_pred = logits.argmax(-1).detach().cpu().numpy()\n",
    "                y_pred = [\n",
    "                    self.sp_lang.inverse_transform(y_pred[:, i])\n",
    "                    for i in range(x.shape[1])\n",
    "                ]\n",
    "\n",
    "                # true labels\n",
    "                y = y.detach().cpu().numpy()\n",
    "                y = [self.sp_lang.inverse_transform(y[:, i]) for i in range(x.shape[1])]\n",
    "\n",
    "                bleu += bleu_score(y_pred, y).item()\n",
    "\n",
    "                bar.set_description(\n",
    "                    f\"Test loss {round(running_loss/step, 3)}, \"\n",
    "                    f\"Test BLEU {round(bleu/step, 3)}\"\n",
    "                )\n",
    "\n",
    "                logs = {\n",
    "                    \"Test loss\": round(running_loss / step, 3),\n",
    "                    \"Test BLEU\": round(bleu / step, 3),\n",
    "                }\n",
    "\n",
    "                return logs\n",
    "\n",
    "    def fit(self, train_loader, test_loader, epochs: int = 1):\n",
    "        \"\"\"\n",
    "        Train and evalaute the model for N epochs\n",
    "\n",
    "        Parameteres:\n",
    "        -----------\n",
    "        train_loader : DataLoader\n",
    "            DataLoader object containing the training data.\n",
    "        test_loader : DataLoader\n",
    "            DataLoader object containing the test data.\n",
    "        epochs: int\n",
    "            Number of epochs to train and evalaute the data loader\n",
    "        \"\"\"\n",
    "\n",
    "        bar = tqdm(range(epochs))\n",
    "\n",
    "        for epoch in bar:\n",
    "            train_logs = self.train_one_epoch(train_loader)\n",
    "            test_logits = self.test_one_epoch(test_loader)\n",
    "\n",
    "            logs = train_logs\n",
    "            logs = logs.update(test_logits)\n",
    "\n",
    "            print(self.translate_sentence(\"the man who sold the world\"))\n",
    "\n",
    "            bar.set_description(logs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98c3ddb1",
   "metadata": {},
   "source": [
    "# Train and Evaluate de NMT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8740db96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53782f949414ea3b53518d72e935ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd773cd0f50a49c2809b69a7f275d6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split data to trian and test\n",
    "train_df, test_df = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# define train and test dataset\n",
    "ds_train = CustomDataset(train_df)\n",
    "ds_test = CustomDataset(test_df, sp_lang=ds_train.sp_lang, en_lang=ds_train.en_lang)\n",
    "\n",
    "# define train and test datalaoder\n",
    "loader_train  = torch.utils.data.DataLoader(ds_train, batch_size=64, num_workers=8, shuffle=True, collate_fn=Lang.right_padding_per_batch)\n",
    "loader_test  = torch.utils.data.DataLoader(ds_test, batch_size=64, num_workers=8, shuffle=False, collate_fn=Lang.right_padding_per_batch)\n",
    "\n",
    "#Define spanish and english vocab size\n",
    "sp_vocab = ds_train.sp_lang.n_words\n",
    "en_vocab = ds_train.en_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89ce8c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f065d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,   481,  4645,    37,     3,    66, 21890,     4,    87,  2760,\n",
       "           12,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "375a504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance de NMT model\n",
    "nmt = NMT(en_vocab, sp_vocab, ds_train.en_lang, ds_train.sp_lang, embedding_dim=300, n_layers=2, hidden_dim=512)\n",
    "nmt.config_model(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "690e10c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f799ed417b455c932db4c1a31ff715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff33ca326ee448fea87b8430fa131941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17a9f9dc8db40d58b29ca97da5e504c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el policía le la la la la ? <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa60c324b004ed2a17b8eaf9551f5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ea00d2a9974629acc5efed09eec59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el hombre hombre el el el mundo . <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1665facaeb84515ab2aeaba2fb7a6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa37ab1647b40bc8b4ff30552a7ec19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el hombre dijo que el el mundo ? <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed603c0bd7ae4371b538d635c5a5bc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e7c1b7402a4e54a75beb5045b1e0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el hombre quién conoce el mundo del mundo . <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68137c5b471f4f45ab4711494945aa06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6f0fe0c24742bab773fe82c1705b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el hombre quién está el todo el mundo ? <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a820a8007f5e4c419f4bb0e4e21e2897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e93dfed52a4d08b8f96b99d9d3ada8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el hombre cree que la mundo más mundo . <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a1700d5d8f48a3a02c465373fac59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f729f5b1efb46dbb00b44615c4bd18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el hombre conocía el mundo . <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182093fd878e4d9a82eb66b0b01b43b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abfd2c3c81740298bdedf1e031ccc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el hombre le panfletos el mundo . <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb58538270ec42e9bd62eb452c416f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a97025ad944f2ca5882d43cff3ebd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el hombre dijo el mundo mundial . <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3dc5e5340d44419aedde31b49cca3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911685f0fd994714829bb69f0f13f7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el hombre que conoce el mundo mundo . <end>\n"
     ]
    }
   ],
   "source": [
    "nmt.fit(loader_train, loader_test, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6caf54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nmt, \"./NMT-GRU.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4d61581",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt = torch.load(\"./NMT-GRU.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "917f30af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------\n",
      "input text:  where's tom's computer?\n",
      "translation:  ¿ dónde está el ordenador de tom ? <end>\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n",
      "input text:  is he your teacher?\n",
      "translation:  ¿ es tu profesor ? <end>\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n",
      "input text:  tom was a coal miner.\n",
      "translation:  tom fue un carbón de carbón . <end>\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n",
      "input text:  it made my mother cry.\n",
      "translation:  borré mi decisión . <end>\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n",
      "input text:  i don't know what happens here.\n",
      "translation:  no sé qué qué aquí aquí aquí . <end>\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, text in dataset.sample(5).iterrows():\n",
    "    \n",
    "    print(\"------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"input text: \",text.english)\n",
    "    print(\"translation: \",nmt.translate_sentence(text.english))\n",
    "    print(\"-----------------------------------------------------------------------------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "0435c29a728a8af6f6141793e2794fa2251761d1c1dcca19e68ebfa0c489a176"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
