{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b061b4b",
   "metadata": {},
   "source": [
    "## Neural Machine Traslation using Encoder-Decoder Architecture\n",
    "\n",
    "The aim of this notebook is to implement a Neural Machine Traslation (NMT) using basic [encoder-decoder](https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf) approach without using attention mechanism\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04eaa0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Donload de dataset\n",
    "# !mkdir MNT-Dataset\n",
    "# !wget -P MNT-Dataset/ https://www.manythings.org/anki/spa-eng.zip\n",
    "# !unzip MNT-Dataset/spa-eng.zip -d MNT-Dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8fc48e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ocastano/.pyenv/versions/3.9.0/envs/development-session/lib/python3.9/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# import libaries\n",
    "import torch\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import multiprocessing as mp\n",
    "\n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchmetrics.functional import bleu_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "282ff818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139636, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go.</td>\n",
       "      <td>ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go.</td>\n",
       "      <td>vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>go.</td>\n",
       "      <td>vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>go.</td>\n",
       "      <td>váyase.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hi.</td>\n",
       "      <td>hola.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english  spanish\n",
       "0     go.      ve.\n",
       "1     go.    vete.\n",
       "2     go.    vaya.\n",
       "3     go.  váyase.\n",
       "4     hi.    hola."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = pd.read_table(\"MNT-Dataset/spa.txt\", header=None, names=[\"english\", \"spanish\", \"ref\"]).drop(labels=[\"ref\"], axis=1)\n",
    "print(dataset.shape)\n",
    "dataset.english = dataset.english.str.lower()\n",
    "dataset.spanish = dataset.spanish.str.lower()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b23113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a tokenizer using spacy\n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self, language: str = None) -> None:\n",
    "        \"\"\"\n",
    "        A simple tokenizer class that uses Spacy to tokenize text.\n",
    "\n",
    "        Parameteres:\n",
    "        ------------\n",
    "            language (str, optional): The language of the text to be tokenized. Defaults to None.\n",
    "                Supported languages are 'sp' for Spanish and 'en' for English.\n",
    "        \"\"\"\n",
    "\n",
    "        if language == \"sp\":\n",
    "            self.nlp = spacy.load(\"es_core_news_sm\")  # load the Spanish Spacy model\n",
    "        elif language == \"en\":\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")  # load the English Spacy model\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Tokenizes a given text using the Spacy tokenizer.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            A list of strings representing the tokens in the text.\n",
    "        \"\"\"\n",
    "\n",
    "        return [w.text for w in self.nlp.tokenizer(text)]  # return the text tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c35175f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we a language class that represents a language and its vocabulary\n",
    "class Lang:\n",
    "    def __init__(self, name:str, language:str=\"sp\"):\n",
    "        \"\"\"\n",
    "        A class for language preprocessing and encoding. It uses a tokenizer to split text into tokens, and encodes\n",
    "        these tokens into integer values. It also provides methods to add sentences and words to the vocabulary, and to\n",
    "        transform text into its encoded form.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        name : str\n",
    "            A name for the language object.\n",
    "        language : str, default='sp'\n",
    "            The language of the text to process. Currently supported languages are 'sp' (Spanish) and 'en' (English).\n",
    "        \"\"\"\n",
    "        \n",
    "        self.name = name\n",
    "        self.language = language\n",
    "        self.word2index = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "        self.tokenizer = Tokenizer(language)\n",
    "\n",
    "    def addSentence(self, sentence:str):\n",
    "        \"\"\"\n",
    "        Add a sentence to the vocabulary.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        sentence : str\n",
    "            The sentence to add.\n",
    "        \"\"\"\n",
    "        \n",
    "        for word in self.tokenizer(sentence):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word:str):\n",
    "        \"\"\"\n",
    "        Add a word to the vocabulary.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        word : str\n",
    "            The word to add.\n",
    "        \"\"\"\n",
    "        \n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def fit(self, dataset:List[str]):\n",
    "        \"\"\"\n",
    "        Build the vocabulary from a dataset.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataset : list\n",
    "            A list of sentences to add to the vocabulary.\n",
    "        \"\"\"\n",
    "        \n",
    "        for data in tqdm(dataset):\n",
    "            self.addSentence(data)\n",
    "\n",
    "    def transform(self, text:str, padding:bool=True):\n",
    "        \"\"\"\n",
    "        Transform text into its encoded form.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The text to encode.\n",
    "        padding : bool, default=True\n",
    "            Whether to pad the sequence to the maximum sequence length.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        encoding : list\n",
    "            A list of integers representing the encoded sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = self.tokenizer(text)\n",
    "        if padding:\n",
    "            tokens = [\"<start>\"] + tokens + [\"<end>\"]\n",
    "            tokens = tokens\n",
    "\n",
    "        encoding = [self.word2index[tk] if tk in self.word2index.keys() else 3 for tk in tokens]\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def inverse_transform(self, tokens:List):\n",
    "        \"\"\"\n",
    "        Decodes the encoded sequence of integers using the vocabulary of the language.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "            tokens: list\n",
    "                The encoded sequence of integers to decode.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            str: The decoded sentence.\n",
    "        \"\"\"\n",
    "        \n",
    "        words = [self.index2word[tk] for tk in tokens]\n",
    "\n",
    "        return \" \".join(words)\n",
    "    \n",
    "    @staticmethod\n",
    "    def right_padding_per_batch(batch: tuple):\n",
    "        \"\"\"\n",
    "        Pads the sequence of tokens with 0s to match the sequence length per batch. \n",
    "        This method will be pass to the collate_fn argument of the Dataloader class.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "            batch: tuple \n",
    "                The sequence of tokens to pad.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            tuple: The padded sequence of tokens in the batch.\n",
    "        \"\"\"\n",
    "    \n",
    "        en_text_bs, sp_text_bs = [], []\n",
    "\n",
    "        for en_text, sp_text in batch:\n",
    "            en_text_bs.append(en_text)\n",
    "            sp_text_bs.append(sp_text)\n",
    "\n",
    "        en_text_bs = pad_sequence(en_text_bs, padding_value=0)\n",
    "        sp_text_bs = pad_sequence(sp_text_bs, padding_value=0)\n",
    "\n",
    "        return en_text_bs, sp_text_bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6ead0",
   "metadata": {},
   "source": [
    "# Data loader\n",
    "\n",
    "We define a custom data loader that output the token for the sentences in spanish and english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9899ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset:pd.DataFrame, sp_lang:Lang=None, en_lang:Lang=None):\n",
    "        \"\"\"\n",
    "        A PyTorch custom dataset for language translation.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        dataset : DataFrame\n",
    "            The dataset containing the English and Spanish sentences.\n",
    "        sp_lang: Lang\n",
    "            The language object for the Spanish language. Default None\n",
    "        en_lang: Lang\n",
    "            The language object for the English language. Default None\n",
    "        \"\"\"\n",
    "    \n",
    "        self.dataset = dataset\n",
    "\n",
    "        if isinstance(sp_lang, Lang) and isinstance(en_lang, Lang):\n",
    "            self.sp_lang = sp_lang\n",
    "            self.en_lang = en_lang\n",
    "            \n",
    "        else:\n",
    "            # Initialize language objects for Spanish and English\n",
    "            self.sp_lang = Lang(\"sp\", language=\"sp\")\n",
    "            self.sp_lang.fit(dataset.spanish)\n",
    "\n",
    "            self.en_lang = Lang(\"en\", language=\"en\")\n",
    "            self.en_lang.fit(dataset.english)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        int\n",
    "            The number of samples in the dataset\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a sample from the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        idx : int\n",
    "            The index of the sample to return.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        tuple of torch.Tensor\n",
    "            The English sentence and the Spanish sentence as tensors.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the Spanish and English sentences from the dataset\n",
    "        sp_text = self.dataset.spanish.tolist()[idx]\n",
    "        en_text = self.dataset.english.tolist()[idx]\n",
    "\n",
    "        # Transform the Spanish and English sentences using the language objects\n",
    "        sp_text = self.sp_lang.transform(sp_text)\n",
    "        en_text = self.en_lang.transform(en_text)\n",
    "\n",
    "        # Convert the transformed sentences to tensors\n",
    "        sp_text = torch.Tensor(sp_text).long()\n",
    "        en_text = torch.Tensor(en_text).long()\n",
    "\n",
    "        return en_text, sp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "350ec051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339baa8f26e3499c92702a17df6769a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139636 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ocastano/.pyenv/versions/3.9.0/envs/development-session/lib/python3.9/site-packages/spacy/util.py:887: UserWarning: [W095] Model 'en_core_web_sm' (3.4.1) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.5.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44d370b279343c08fd416be76b7fedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139636 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the dataloader\n",
    "ds_train = CustomDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbe3ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the spanish and English vocab size\n",
    "sp_vocab = ds_train.sp_lang.n_words\n",
    "en_vocab = ds_train.en_lang.n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3e494",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8220d8e",
   "metadata": {},
   "source": [
    "As encoder we use a GRU layer that returns the hidden state\n",
    "\n",
    "$$O_t, h_t = GRU(x_t, h_{t-1})$$\n",
    "\n",
    "\n",
    "Here, $O_t$ is the output at time step $t$, $h_t$ is the hidden state at time step t, $x_t$ is the input at time step $t$, and $h_{t-1}$ is the hidden state from the previous time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a12ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_dim=100, n_layers=2, hidden_dim=10, dropout=0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Encode input sequences using a GRU-based neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : int\n",
    "            The size of the input vocabulary.\n",
    "        embedding_dim : int, optional (default=100)\n",
    "            The dimension of the input word embeddings.\n",
    "        n_layers : int, optional (default=2)\n",
    "            The number of GRU layers.\n",
    "        hidden_dim : int, optional (default=10)\n",
    "            The dimension of the hidden state of the GRU.\n",
    "        dropout : float, optional (default=0.5)\n",
    "            The dropout probability to use in the GRU and embedding layers.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding layer\n",
    "        # input shape: either [batch_size, seq_len] or [seq_len, batch_size]\n",
    "        self.embeddings = nn.Embedding(input_size, embedding_dim)\n",
    "\n",
    "        # GRU layers\n",
    "        # input shape: [batch_size, seq_len, features] if batch_first=True\n",
    "        # input shape: [seq_len, batch_size, features] if batch_first=False\n",
    "        self.rnn = nn.GRU(\n",
    "            embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=False\n",
    "        )\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the forward pass of the encoder on input `x`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor with shape (seq_len, batch_size).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor with shape (seq_len, batch_size, num_directions * hidden_dim).\n",
    "        torch.Tensor\n",
    "            The hidden state tensor with shape (n_layers * n_directions, batch_size, hidden_dim).\n",
    "        \"\"\"\n",
    "        # x shape: [seq_len, batch_size]\n",
    "\n",
    "        # Embedding Layer\n",
    "        # output shape: [seq_len, batch_size, emb_dim]\n",
    "        x = self.dropout(self.embeddings(x))\n",
    "\n",
    "        # GRU Layer\n",
    "        # output(x) shape: [seq_len, batch_size, num_directions * hidden_dim]\n",
    "        # output(hidden) shape: = [n_layers * n_directions, batch_size, hidden_dim]\n",
    "        x, hidden = self.rnn(x)\n",
    "        return x, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4810493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 10])\n",
      "torch.Size([2, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test encoder\n",
    "encoder = Encoder(100)\n",
    "\n",
    "x = torch.tensor([[1, 3, 4], [4, 5, 6]], dtype=torch.long)\n",
    "x, hidden = encoder(x.T)\n",
    "print(x.shape)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a2c06",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "The decoder (d) is responsible for generating the translated output sentence in the target language. In this notebook, we will be using a simple decoder architecture that consists of a single GRU layer followed by a multi-layer perceptron (MLP) that outputs logits. The decoder takes as input the previously generated word $y_{t-1}$, the previous hidden state $s_{t-1}$, and the context vector $c$, which is calculated by the encoder. The output of the decoder at each time step $t$ is a set of logits $logits_t$, which are used to compute the probability distribution over the target vocabulary. The decoder also updates its own hidden state $s_t$ at each time step, which is used to condition the generation of subsequent words in the output sequence.\n",
    "\n",
    "The equations for the decoder can be written as follows:\n",
    "\n",
    "$$logits_t, s_t = d(y_{t-1}, s_{t-1}, c)$$\n",
    "\n",
    "where $y_{t-1}$ is the previously generated word, $s_{t-1}$ is the previous hidden state of the decoder, and $c$ is the context vector computed by the encoder. The output of the decoder at time step $t$ is represented by $logits_t$, and the updated hidden state is represented by $s_t$. The computation of the context vector is described in the encoder section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6320c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        embedding_dim: int = 100,\n",
    "        n_layers: int = 2,\n",
    "        hidden_dim: int = 10,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A class representing a decoder in a sequence-to-sequence model.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            The size of the vocabulary of the input language.\n",
    "        output_size : int\n",
    "            The size of the vocabulary of the output language.\n",
    "        embedding_dim : int\n",
    "            The dimensionality of the embeddings for the input language. Default is 100.\n",
    "        n_layers : int\n",
    "            The number of layers in the GRU. Default is 2.\n",
    "        hidden_dim : int\n",
    "            The number of features in the GRU. Default is 10.\n",
    "        dropout : float\n",
    "            The probability of dropping out a neuron. Default is 0.5.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding layer\n",
    "        # input shape: either [batch_size, seq_len] or [seq_len, batch_size]\n",
    "        self.embeddings = nn.Embedding(input_size, embedding_dim)\n",
    "\n",
    "        # GRU layers\n",
    "        # input shape: [batch_size, seq_len, features] if batch_first=True\n",
    "        # input shape: [seq_len, batch_size, features] if batch_first=False\n",
    "        self.rnn = nn.GRU(\n",
    "            embedding_dim + hidden_dim,\n",
    "            hidden_dim,\n",
    "            n_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # Fully conected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden, context):\n",
    "        \"\"\"\n",
    "        Perform forward pass of the decoder.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor with shape [seq_len=1, batch_size].\n",
    "        hidden : torch.Tensor\n",
    "            The hidden state of the GRU with shape [n_layers * n_directions, batch_size, hid_dim].\n",
    "        context : torch.Tensor\n",
    "            The context vector with shape [1, batch_size, hid_dim].\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        x : torch.Tensor\n",
    "            The output tensor with shape [batch_size, output_size].\n",
    "        hidden : torch.Tensor\n",
    "            The updated hidden state of the GRU with shape [n_layers * n_directions, batch_size, hidden_dim].\n",
    "        \"\"\"\n",
    "\n",
    "        # x shape: [seq_len=1, batch_size]\n",
    "        # hidden shape: = [n_layers * n_directions, batch_size, hid_dim]\n",
    "        # context shape: [1, batch_size, hid_dim]\n",
    "\n",
    "        # Embeddings\n",
    "        # output shape: [seq_len=1, batch_size, emb_dim]\n",
    "        x = self.dropout(self.embeddings(x))\n",
    "\n",
    "        # Concatenation\n",
    "        # output shape: [seq_len, batch_size, emb_dim + hid_dim]\n",
    "        x = torch.cat((x, context), dim=2)\n",
    "\n",
    "        # GRU Layer\n",
    "        # output(x) shape: [seq_len=1, batch_size, num_directions * hidden_dim]\n",
    "        # output(hidden) shape: = [n_layers * n_directions, batch_size, hidden_dim]\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Fully connected layer\n",
    "        # output shape: [batch_size, output_size]\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a90a6bc",
   "metadata": {},
   "source": [
    "### Test the decoder\n",
    "\n",
    "Now we test that the decoder is working as we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f872469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 100])\n",
      "torch.Size([2, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "# test Decoder class\n",
    "bs = 2\n",
    "hidden_dim = 10\n",
    "seq_len = 3\n",
    "n_layers = 2\n",
    "\n",
    "hidden = torch.rand((n_layers, bs, hidden_dim))\n",
    "context = torch.rand((1, bs, hidden_dim))\n",
    "x = torch.tensor([[1], [4]], dtype=torch.long)\n",
    "\n",
    "decoder = Decoder(100, output_size=100)\n",
    "x, hidden = decoder(x.T, hidden, context)\n",
    "print(x.shape)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365abdde",
   "metadata": {},
   "source": [
    "# Neuaral Machine Transalation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92f977",
   "metadata": {},
   "source": [
    "Now we build a Neural Machine Translation using the encoder-decoder approach without using the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "927327a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        en_vocab,\n",
    "        sp_vocab,\n",
    "        en_lang,\n",
    "        sp_lang,\n",
    "        embedding_dim,\n",
    "        n_layers=2,\n",
    "        hidden_dim=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Neural Machine Translation model (NMT) based on encoder-decoder architecture without attention.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        en_vocab : int\n",
    "            Size of the English vocabulary.\n",
    "        sp_vocab : int\n",
    "            Size of the Spanish vocabulary.\n",
    "        en_lang : object\n",
    "            English language object.\n",
    "        sp_lang : object\n",
    "            Spanish language object.\n",
    "        embedding_dim : int\n",
    "            Dimension of the word embedding space.\n",
    "        n_layers : int, optional\n",
    "            Number of layers in the encoder and decoder (default is 2).\n",
    "        hidden_dim : int, optional\n",
    "            Dimension of the hidden state in the encoder and decoder (default is 10).\n",
    "        \"\"\"\n",
    "\n",
    "        self.en_vocab = en_vocab\n",
    "        self.sp_vocab = sp_vocab\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sp_lang = sp_lang\n",
    "        self.en_lang = en_lang\n",
    "\n",
    "        # Define Encoder and Decoder\n",
    "        self.encoder = Encoder(en_vocab, embedding_dim, n_layers, hidden_dim)\n",
    "        self.decoder = Decoder(sp_vocab, sp_vocab, embedding_dim, n_layers, hidden_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Forward pass of the NMT model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : tensor\n",
    "            Tensor of shape (seq_len, batch_size) containing the input sequences in English.\n",
    "        y : tensor\n",
    "            Tensor of shape (seq_len, batch_size) containing the input sequences in Spanish.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : tensor\n",
    "            Tensor of shape (seq_len, batch_size, sp_vocab) containing the predicted Spanish sequences.\n",
    "        \"\"\"\n",
    "\n",
    "        # shape x: [seq_len, batch_size]\n",
    "        # shape y: [seq_len, batch_size]\n",
    "\n",
    "        target_len = y.shape[0]\n",
    "        batch_size = x.shape[1]\n",
    "\n",
    "        # outputs tensor\n",
    "        # output shape: [seq_len, batch_size, vocab_size]\n",
    "        outputs = torch.zeros(target_len, batch_size, self.sp_vocab).to(self.device)\n",
    "\n",
    "        # Encoder\n",
    "        # output(y_encoder) shape: [seq_len, batch_size, num_directions * hidden_dim]\n",
    "        # output(hidden) shape: = [n_layers * n_directions, batch_size, hidden_dim]\n",
    "        y_encoder, hidden = self.encoder(x)\n",
    "\n",
    "        # Initial prediction\n",
    "        # output shape: [1, batch_size]\n",
    "        x_decoder = y[[0], :]\n",
    "\n",
    "        # context vector\n",
    "        # output(hidden) shape: [1, batch_size, num_directions * hidden_size]\n",
    "        context = y_encoder[[-1], :, :]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # output(output) shape: [batch_size, output_size]\n",
    "            # output(hidden) shape: = [n_layers * n_directions, batch_size, hidden_dim]\n",
    "            output, hidden = self.decoder(x_decoder, hidden, context)\n",
    "            outputs[[t], :, :] = output\n",
    "            y_decoder = output.argmax(-1)\n",
    "            x_decoder = y[[t], :] if np.random.random() < 0.5 else y_decoder\n",
    "\n",
    "        # output shape: [seq_len, batch_size, vocab_size]\n",
    "        return outputs\n",
    "\n",
    "    def translate_sentence(self, x):\n",
    "        \"\"\"\n",
    "        Translate an English sentence to Spanish.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : str\n",
    "            English sentence to translate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : list\n",
    "            List of integers representing the predicted Spanish sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        # Transform input text to tokens\n",
    "        x = (\n",
    "            torch.Tensor(self.en_lang.transform(x))\n",
    "            .long()\n",
    "            .reshape(-1, 1)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        # define output array\n",
    "        outputs = []\n",
    "\n",
    "        # Initial token <start>\n",
    "        x_decoder = torch.Tensor([[1]]).long().to(self.device)\n",
    "\n",
    "        # pass sentence to the encoder\n",
    "        y_encoder, hidden = self.encoder(x)\n",
    "\n",
    "        # Define cotext vector\n",
    "        context = y_encoder[[-1], :, :]\n",
    "\n",
    "        t = 1\n",
    "\n",
    "        # this will run until prediction is <end> or t >= 200\n",
    "        while x_decoder != 2:\n",
    "            output, hidden = self.decoder(x_decoder, hidden, context)\n",
    "            outputs.append(output.argmax(-1).item())\n",
    "            x_decoder = y_decoder = output.argmax(-1)\n",
    "\n",
    "            if t >= 200:\n",
    "                break\n",
    "\n",
    "            t += 1\n",
    "\n",
    "        return self.sp_lang.inverse_transform(outputs)\n",
    "\n",
    "    def config_model(self, device: str = \"cuda\"):\n",
    "        \"\"\"\n",
    "        Configure the NMT model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        device : str, optional\n",
    "            Device to use (default is \"cuda\").\n",
    "        \"\"\"\n",
    "\n",
    "        # define device to operate\n",
    "        self.device = device\n",
    "\n",
    "        # set model's device\n",
    "        self.to(self.device)\n",
    "\n",
    "        # define loss function\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "        # define optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Train the NMT model for one epoch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_loader : DataLoader\n",
    "            DataLoader object containing the training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logs : dict\n",
    "            Dictionary containing the training loss and BLEU score.\n",
    "        \"\"\"\n",
    "\n",
    "        running_loss = 0\n",
    "        bleu = 0\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        bar = tqdm(train_loader, leave=True)\n",
    "\n",
    "        for step, (x, y) in enumerate(bar, 1):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # set device\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "            # forward pass\n",
    "            logits = self(x, y)  # shape: [seq_len, batch_size, vocab_size]\n",
    "\n",
    "            # Remove <start> from target\n",
    "            y = y[1:]\n",
    "            logits = logits[1:]\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.loss(logits.reshape(-1, logits.shape[2]), y.reshape(-1))\n",
    "\n",
    "            # Clip the gradient value is it exceeds > 1\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1)\n",
    "\n",
    "            # Compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weigths\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # compute running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # predictions\n",
    "            y_pred = logits.argmax(-1).detach().cpu().numpy()\n",
    "            y_pred = [\n",
    "                self.sp_lang.inverse_transform(y_pred[:, i]) for i in range(x.shape[1])\n",
    "            ]\n",
    "\n",
    "            # true labels\n",
    "            y = y.detach().cpu().numpy()\n",
    "            y = [self.sp_lang.inverse_transform(y[:, i]) for i in range(x.shape[1])]\n",
    "\n",
    "            bleu += bleu_score(y_pred, y).item()\n",
    "\n",
    "            bar.set_description(\n",
    "                f\"Train loss {round(running_loss/step, 3)}, \"\n",
    "                f\"Train BLEU {round(bleu/step, 3)}\"\n",
    "            )\n",
    "\n",
    "        logs = {\n",
    "            \"Train loss\": round(running_loss / step, 3),\n",
    "            \"Train BLEU\": round(bleu / step, 3),\n",
    "        }\n",
    "\n",
    "        return logs\n",
    "\n",
    "    def test_one_epoch(self, test_loader):\n",
    "        \"\"\"\n",
    "        Test the NMT model for one epoch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_loader : DataLoader\n",
    "            DataLoader object containing the test data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logs : dict\n",
    "            Dictionary containing the test loss and BLEU score.\n",
    "        \"\"\"\n",
    "\n",
    "        running_loss = 0\n",
    "        bleu = 0\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bar = tqdm(test_loader, leave=True)\n",
    "\n",
    "            for step, (x, y) in enumerate(bar, 1):\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # set device\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                # forward pass\n",
    "                logits = self(x, y)  # shape: [seq_len, batch_size, vocab_size]\n",
    "\n",
    "                # Remove <start> from target\n",
    "                y = y[1:]\n",
    "                logits = logits[1:]\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.loss(logits.reshape(-1, logits.shape[2]), y.reshape(-1))\n",
    "\n",
    "                # compute running loss\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # predictions\n",
    "                y_pred = logits.argmax(-1).detach().cpu().numpy()\n",
    "                y_pred = [\n",
    "                    self.sp_lang.inverse_transform(y_pred[:, i])\n",
    "                    for i in range(x.shape[1])\n",
    "                ]\n",
    "\n",
    "                # true labels\n",
    "                y = y.detach().cpu().numpy()\n",
    "                y = [self.sp_lang.inverse_transform(y[:, i]) for i in range(x.shape[1])]\n",
    "\n",
    "                bleu += bleu_score(y_pred, y).item()\n",
    "\n",
    "                bar.set_description(\n",
    "                    f\"Test loss {round(running_loss/step, 3)}, \"\n",
    "                    f\"Test BLEU {round(bleu/step, 3)}\"\n",
    "                )\n",
    "\n",
    "                logs = {\n",
    "                    \"Test loss\": round(running_loss / step, 3),\n",
    "                    \"Test BLEU\": round(bleu / step, 3),\n",
    "                }\n",
    "\n",
    "                return logs\n",
    "\n",
    "    def fit(self, train_loader, test_loader, epochs: int = 1):\n",
    "        \"\"\"\n",
    "        Train and evalaute the model for N epochs\n",
    "\n",
    "        Parameteres:\n",
    "        -----------\n",
    "        train_loader : DataLoader\n",
    "            DataLoader object containing the training data.\n",
    "        test_loader : DataLoader\n",
    "            DataLoader object containing the test data.\n",
    "        epochs: int\n",
    "            Number of epochs to train and evalaute the data loader\n",
    "        \"\"\"\n",
    "\n",
    "        bar = tqdm(range(epochs))\n",
    "\n",
    "        for epoch in bar:\n",
    "            train_logs = self.train_one_epoch(train_loader)\n",
    "            test_logits = self.test_one_epoch(test_loader)\n",
    "\n",
    "            logs = train_logs\n",
    "            logs = logs.update(test_logits)\n",
    "\n",
    "            print(self.translate_sentence(\"the man who sold the world\"))\n",
    "\n",
    "            bar.set_description(logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e9e03c",
   "metadata": {},
   "source": [
    "# Train and Evaluate de NMT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57a88586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbd26bee7e645908759f8fbbfd144c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ocastano/.pyenv/versions/3.9.0/envs/development-session/lib/python3.9/site-packages/spacy/util.py:887: UserWarning: [W095] Model 'en_core_web_sm' (3.4.1) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.5.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72aa760e4d7c4780a156082e835aa324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split data to trian and test\n",
    "train_df, test_df = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# define train and test dataset\n",
    "ds_train = CustomDataset(train_df)\n",
    "ds_test = CustomDataset(test_df, sp_lang=ds_train.sp_lang, en_lang=ds_train.en_lang)\n",
    "\n",
    "# define train and test datalaoder\n",
    "loader_train  = torch.utils.data.DataLoader(ds_train, batch_size=64, num_workers=8, shuffle=True, collate_fn=Lang.right_padding_per_batch)\n",
    "loader_test  = torch.utils.data.DataLoader(ds_test, batch_size=64, num_workers=8, shuffle=False, collate_fn=Lang.right_padding_per_batch)\n",
    "\n",
    "#Define spanish and english vocab size\n",
    "sp_vocab = ds_train.sp_lang.n_words\n",
    "en_vocab = ds_train.en_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ec405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance de NMT model\n",
    "nmt = NMT(en_vocab, sp_vocab, ds_train.en_lang, ds_train.sp_lang, embedding_dim=300, n_layers=2, hidden_dim=512)\n",
    "nmt.config_model(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc773132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the NMT model\n",
    "nmt.fit(loader_train, loader_test, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cf91a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124732be5afc4527976103e0950ed959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Test loss': 1.68, 'Test BLEU': 0.195}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model performance on train set\n",
    "nmt.test_one_epoch(loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35c37f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c372d78eb904110bc9b12c1aaf7ada7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Test loss': 2.216, 'Test BLEU': 0.126}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model performance on test set\n",
    "nmt.test_one_epoch(loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f93babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(nmt, \"NMT-GRU-no-attention.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bbb7b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "nmt = torch.load(\"NMT-GRU-no-attention.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac29064e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'el hombre dijo el mundo . <end>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the NMT with dummy a sentence\n",
    "nmt.translate_sentence(\"the man who sold the world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd55c9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------\n",
      "input text:  tom is just like me.\n",
      "translation:  tom es como yo . <end>\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n",
      "input text:  i have the ace of hearts.\n",
      "translation:  tengo as de picas . <end>\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n",
      "input text:  if you drive carefully you'll avoid accidents.\n",
      "translation:  si te vas con evitarás evitarás evitarás evitarás . <end>\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n",
      "input text:  did you lose consciousness after the accident?\n",
      "translation:  perdiste perdiste después del accidente accidente ? <end>\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n",
      "input text:  i think you should eat a ham sandwich.\n",
      "translation:  creo que deberías comer un sándwich . <end>\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test more translations\n",
    "for _, text in dataset.sample(5).iterrows():\n",
    "    \n",
    "    print(\"------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"input text: \",text.english)\n",
    "    print(\"translation: \",nmt.translate_sentence(text.english))\n",
    "    print(\"-----------------------------------------------------------------------------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "0435c29a728a8af6f6141793e2794fa2251761d1c1dcca19e68ebfa0c489a176"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
