{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c08a40b",
   "metadata": {},
   "source": [
    "# Neural Machine Traslation using Encoder-Decoder Architecture\n",
    "\n",
    "The aim of this notebook is to implement a Neural Machine Traslation (NMT) using basic [encoder-decoder](https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf) approach with [Bahandanau attention mechanism](https://arxiv.org/pdf/1409.0473.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23eb3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !mkdir MNT-Dataset\n",
    "# !wget -P MNT-Dataset/ https://www.manythings.org/anki/spa-eng.zip\n",
    "# !unzip MNT-Dataset/spa-eng.zip -d MNT-Dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842c663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaries\n",
    "import torch\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch import nn\n",
    "import multiprocessing as mp\n",
    "\n",
    "from typing import List\n",
    "from torchmetrics.functional import bleu_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d9132c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141543, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Váyase.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corre!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corran!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Huye!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corra!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corred!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english   spanish\n",
       "0     Go.       Ve.\n",
       "1     Go.     Vete.\n",
       "2     Go.     Vaya.\n",
       "3     Go.   Váyase.\n",
       "4     Hi.     Hola.\n",
       "5    Run!   ¡Corre!\n",
       "6    Run!  ¡Corran!\n",
       "7    Run!    ¡Huye!\n",
       "8    Run!   ¡Corra!\n",
       "9    Run!  ¡Corred!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = pd.read_table(\"MNT-Dataset/spa.txt\", header=None, names=[\"english\", \"spanish\", \"ref\"]).drop(labels=[\"ref\"], axis=1)\n",
    "print(dataset.shape)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8357446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic preprocessing.\n",
    "def preprocess(text: str):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove accents\n",
    "    # text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # replace numbers by %num\n",
    "    text = re.sub(r'\\d+', '%num', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa71a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a tokenizer using spacy\n",
    "class Tokenizer:\n",
    "    def __init__(self, language: str = None) -> None:\n",
    "        \"\"\"\n",
    "        A simple tokenizer class that uses Spacy to tokenize text.\n",
    "\n",
    "        Parameteres:\n",
    "        ------------\n",
    "            language (str, optional): The language of the text to be tokenized. Defaults to None.\n",
    "                Supported languages are 'sp' for Spanish and 'en' for English.\n",
    "        \"\"\"\n",
    "\n",
    "        if language == \"sp\":\n",
    "            self.nlp = spacy.load(\"es_core_news_sm\")  # load the Spanish Spacy model\n",
    "        elif language == \"en\":\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")  # load the English Spacy model\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Tokenizes a given text using the Spacy tokenizer.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            A list of strings representing the tokens in the text.\n",
    "        \"\"\"\n",
    "\n",
    "        return [w.text for w in self.nlp.tokenizer(text)]  # return the text tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d635042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we a language class that represents a language and its vocabulary\n",
    "class Lang:\n",
    "    def __init__(self, language: str = \"sp\", sequence_length: int = 50):\n",
    "        \"\"\"\n",
    "        A class for language preprocessing and encoding. It uses a tokenizer to split text into tokens, and encodes\n",
    "        these tokens into integer values. It also provides methods to add sentences and words to the vocabulary, and to\n",
    "        transform text into its encoded form.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        language : str, default='sp'\n",
    "            The language of the text to process. Currently supported languages are 'sp' (Spanish) and 'en' (English).\n",
    "        sequence_length : int, default=50\n",
    "            The maximum length of a sequence of tokens. Longer sequences are truncated and shorter ones are padded.\n",
    "        \"\"\"\n",
    "\n",
    "        self.language = language\n",
    "        self.sequence_length = sequence_length\n",
    "        self.word2index = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "        self.tokenizer = Tokenizer(language)\n",
    "\n",
    "    def addSentence(self, sentence: str):\n",
    "        \"\"\"\n",
    "        Add a sentence to the vocabulary.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        sentence : str\n",
    "            The sentence to add.\n",
    "        \"\"\"\n",
    "\n",
    "        for word in self.tokenizer(sentence):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word: str):\n",
    "        \"\"\"\n",
    "        Add a word to the vocabulary.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        word : str\n",
    "            The word to add.\n",
    "        \"\"\"\n",
    "\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def fit(self, dataset: List[str]):\n",
    "        \"\"\"\n",
    "        Build the vocabulary from a dataset.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataset : list\n",
    "            A list of sentences to add to the vocabulary.\n",
    "        \"\"\"\n",
    "\n",
    "        for data in tqdm(dataset):\n",
    "            self.addSentence(preprocess(data))\n",
    "\n",
    "    def transform(self, text: str, padding: bool = True, start_token: bool = False, end_token: bool = False):\n",
    "        \"\"\"\n",
    "        Transform text into its encoded form.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The text to encode.\n",
    "        padding : bool, default=True\n",
    "            Whether to pad the sequence to the maximum sequence length.\n",
    "        start_token : bool, default=False\n",
    "            Whether to add a start token to the sequence.\n",
    "        end_token : bool, default=False\n",
    "            Whether to add an end token to the sequence.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        encoding : list\n",
    "            A list of integers representing the encoded sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        text = preprocess(text)\n",
    "        tokens = self.tokenizer(text)\n",
    "\n",
    "        if start_token:\n",
    "            tokens = [\"<start>\"] + tokens\n",
    "        if end_token:\n",
    "            tokens = tokens + [\"<end>\"] \n",
    "        if padding:\n",
    "            tokens = self.right_padding(tokens, self.sequence_length)\n",
    "\n",
    "        encoding = [self.word2index[tk] if tk in self.word2index.keys() else 3 for tk in tokens]\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def inverse_transform(self, tokens: List):\n",
    "        \"\"\"\n",
    "        Decodes the encoded sequence of integers using the vocabulary of the language.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "            tokens: list\n",
    "                The encoded sequence of integers to decode.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            str: The decoded sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        words = [self.index2word[tk] for tk in tokens]\n",
    "\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def right_padding(self, tokens: List, sequence_length: int):\n",
    "        \"\"\"\n",
    "        Pads the sequence of tokens with <pad> tokens to match the desired sequence length.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "            tokens: list\n",
    "                The sequence of tokens to pad.\n",
    "            sequence_length: int\n",
    "                The desired length of the padded sequence.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            list: The padded sequence of tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(tokens) < sequence_length:\n",
    "            padded_tokens = tokens + [\"<pad>\"] * (sequence_length - len(tokens))\n",
    "\n",
    "        elif len(tokens) == sequence_length:\n",
    "            padded_tokens = tokens[: sequence_length + 1]\n",
    "\n",
    "        elif len(tokens) > sequence_length:\n",
    "            padded_tokens = tokens[: sequence_length] +  [\"<end>\"]\n",
    "\n",
    "        return padded_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd227107",
   "metadata": {},
   "source": [
    "# Custom Dataset\n",
    "\n",
    "We define a custom data set that output the token for the sentences in spanish and english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abbf6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset: pd.DataFrame, max_seq_len: int, src_lang: Lang = None, trg_lang: Lang = None):\n",
    "        \"\"\"\n",
    "        A PyTorch custom dataset for language translation.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "            dataset : pandas DataFrame\n",
    "                The dataset containing the English and Spanish sentences.\n",
    "            max_seq_len : int\n",
    "                The maximum length of the source and target sentences.\n",
    "            src_lang : Lang\n",
    "                The language object for the source language.\n",
    "            trg_lang : Lang\n",
    "                The language object for the target language.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        if isinstance(src_lang, Lang) and isinstance(trg_lang, Lang):\n",
    "            self.src_lang = src_lang\n",
    "            self.trg_lang = trg_lang\n",
    "\n",
    "        else:\n",
    "            # Initialize language objects for Spanish and English\n",
    "            self.src_lang = Lang(language=\"en\", sequence_length=max_seq_len)\n",
    "            self.src_lang.fit(dataset.english)\n",
    "\n",
    "            self.trg_lang = Lang(language=\"sp\", sequence_length=max_seq_len)\n",
    "            self.trg_lang.fit(dataset.spanish)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        int\n",
    "            The number of samples in the dataset\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a sample from the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        idx : int\n",
    "            The index of the sample to return.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        tuple of torch.Tensor\n",
    "            The English sentence and the Spanish sentence as tensors.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the Spanish and English sentences from the dataset\n",
    "        src_text = self.dataset.english.tolist()[idx]\n",
    "        trg_text = self.dataset.spanish.tolist()[idx]\n",
    "\n",
    "        # Transform the Spanish and English sentences using the language objects\n",
    "        src_text = self.src_lang.transform(src_text, start_token=True, end_token=True)\n",
    "        trg_input_text = self.trg_lang.transform(trg_text, start_token=True)\n",
    "        trg_output_text = self.trg_lang.transform(trg_text, end_token=True)\n",
    "\n",
    "        # Convert the transformed sentences to tensors\n",
    "        src_text = torch.Tensor(src_text).long()\n",
    "        trg_input_text = torch.Tensor(trg_input_text).long()\n",
    "        trg_output_text = torch.Tensor(trg_output_text).long()\n",
    "\n",
    "        return src_text, trg_input_text, trg_output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a496826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbab4410144f4263992df8cce4eb3cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/141543 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22865b0f28994fd993959568854c3ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/141543 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1, 4, 2, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([1, 4, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([4, 2, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the dataloader\n",
    "ds_train = CustomDataset(dataset, 10)\n",
    "next(iter(ds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99141af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14213\n",
      "27993\n"
     ]
    }
   ],
   "source": [
    "# get the spanish and English vocab size\n",
    "src_vocab = ds_train.src_lang.n_words; print(src_vocab)\n",
    "trg_vocab = ds_train.trg_lang.n_words; print(trg_vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29aca3ca",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7d2948d",
   "metadata": {},
   "source": [
    "Next, we will construct the transformer model, which consists of the essential components listed below:\n",
    "1. Word Embedding\n",
    "2. Positional Encoding \n",
    "3. Multi-Head Attention \n",
    "4. Add and Normalize \n",
    "5. Point Wise Fully Connected "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1923f57",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/ZdQnGV5.png\" alt= “” width=\"500px\" height=\"700px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5ad4f92",
   "metadata": {},
   "source": [
    "## Position Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "612e2d30",
   "metadata": {},
   "source": [
    "If the length of the sentence is given by $pos$ and the embedding dimension/depth is given by $dim$, positional encoding $\\mathbf{P}$ is a 2-d matrix of same dimension, i.e., $\\mathbf{P} \\in \\mathbb{R}^{l \\times d}$. Every position can be represented with equation in terms of $i$ which is along the $pos$ and $j$ which is along the $dim$ dimension as\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\mathbf{P}_{i, 2 j}=\\sin \\left(i / 1000^{2 j / dim}\\right) \\\\\n",
    "\\mathbf{P}_{i, 2 j+1}=\\cos \\left(i / 1000^{2 j / dim}\\right)\n",
    "\\end{gathered}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9bd5c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403],\n",
       "        [ 0.9093, -0.4161],\n",
       "        [ 0.1411, -0.9900],\n",
       "        [-0.7568, -0.6536]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_len, embed_dim, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        P = torch.zeros(self.seq_len, self.embed_dim)\n",
    "        pos = torch.arange(self.seq_len).reshape(-1, 1)\n",
    "        dim = torch.arange(self.embed_dim).reshape(1, -1)\n",
    "\n",
    "        P[:, 0::2] = torch.sin(pos / 10000 ** (dim[:, 0::2] / self.embed_dim))\n",
    "        P[:, 1::2] = torch.cos(pos / 10000 ** ((dim[:, 1::2] - 1) / self.embed_dim))\n",
    "\n",
    "        return P.to(self.device)\n",
    "\n",
    "# test positional embedding\n",
    "positional_encoding = PositionalEncoding(seq_len=5, embed_dim=2)\n",
    "positional_encoding()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "469d9c4d",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da263307",
   "metadata": {},
   "source": [
    "Here we construct the embedding class which include the token embedding and the positional encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3dfdc27",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/c7EOe74.png\" alt= “” width=\"300px\" height=\"200px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58f7427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding class using positional and token embeddings\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_len, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_dim, device=device)\n",
    "        self.positional_encodings = PositionalEncoding(max_seq_len, embed_dim, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_embeddings = self.token_embeddings(x)\n",
    "        positional_encodings = self.positional_encodings()\n",
    "\n",
    "        embedding = token_embeddings + positional_encodings\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1a11339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Embedding class\n",
    "vocab_size = 10\n",
    "embed_dim = 8\n",
    "max_seq_len = 3\n",
    "bs = 2\n",
    "\n",
    "x = torch.randint(0, vocab_size-1, size=(bs, max_seq_len))\n",
    "\n",
    "embedding = Embedding(vocab_size, embed_dim, max_seq_len)\n",
    "embedding(x).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b994d27f",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "The Transformer paper introduced the multi-head attention layer as a significant and innovative concept."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a490bea",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/I8ouVdr.png\" alt= “” width=\"700px\" height=\"450px\">\n",
    "\n",
    "The output of the i-th head is given by \n",
    "\n",
    "$$\\text { head }_i=\\operatorname{attention}\\left(\\mathbf{W}_q^i \\mathbf{Q}, \\mathbf{W}_k^i \\mathbf{K}, \\mathbf{W}_v^i \\mathbf{V}\\right)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\operatorname{attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q K}^{\\mathrm{T}}}{\\sqrt{d_k}}\\right) \\mathbf{V}$$\n",
    "\n",
    "Multi-head attention have multiple sets of query/key/value weight matrices, each resulting in different query/key/value matrices for the inputs, finally generating output matrices $z_i$ . These output matrices from each head are concatenated and multiplied with an additional weight matrix, $W_O$ , to get a single final matrix, $Z$, with vectors zi as output for each input $x_i$ .The MultiHead\n",
    "\n",
    "$$\\operatorname{multihead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})=\\mathbf{W}_O \\text { concat }\\left(\\text { head }_1, \\ldots, \\text { head }_h\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6ab691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define MultiHeadAttention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_head = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.w_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.w_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.w_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.w_o = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q shape: (batch_size, seq_len, embed_dim)\n",
    "        # k shape: (batch_size, seq_len, embed_dim)\n",
    "        # v shape: (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        batch_size = q.shape[0]\n",
    "        seq_len = q.shape[1]\n",
    "\n",
    "        Q = self.w_q(q)  # Q shape : (batch_size, seq_len, embed_dim)\n",
    "        K = self.w_k(k)  # K shape : (batch_size, seq_len, embed_dim)\n",
    "        V = self.w_v(v)  # V shape : (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        Q = Q.reshape(batch_size, seq_len, self.num_head, self.head_dim).permute(0, 2, 1, 3)  # Q shape: (batch_size, num_head, seq_len, head_dim)\n",
    "        K = K.reshape(batch_size, seq_len, self.num_head, self.head_dim).permute(0, 2, 1, 3)  # K shape: (batch_size, num_head, seq_len, head_dim)\n",
    "        V = V.reshape(batch_size, seq_len, self.num_head, self.head_dim).permute(0, 2, 1, 3)  # V shape: (batch_size, num_head, seq_len, head_dim)\n",
    "\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2))/np.sqrt(self.head_dim) # energy shape: (batch_size, num_head, seq_len, seq_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -float('inf'))\n",
    "\n",
    "        attention = torch.matmul(F.softmax(energy, dim=-1), V)   # attention shape: (batch_size, num_head, seq_len, head_dim)\n",
    "\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        Z = self.w_o(attention.permute(0, 2, 1, 3).reshape(batch_size, seq_len, self.num_head * self.head_dim)) # Z shape: (batch_size, seq_len, embed_dim)\n",
    " \n",
    "        return Z, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7c9997e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the MultiHeadAttention\n",
    "bs = 2\n",
    "embed_dim = 4\n",
    "max_length = 5\n",
    "num_heads = 1\n",
    "\n",
    "X = torch.rand(size=(bs, max_length, embed_dim))\n",
    "mask = torch.tril(torch.ones(max_length, max_length))\n",
    "multi_head_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "z, attention = multi_head_attention(X, X, X, mask)\n",
    "z.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8debf03",
   "metadata": {},
   "source": [
    "### Residuals and Layer Normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52be866f",
   "metadata": {},
   "source": [
    "Similar to ResNets, the inputs, $X$, are short circuited to the output, $Z$, and both are added and passed through layer normalization $$addAndN orm(X + Z)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d678779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define add and normalize layer\n",
    "class AddAndNormalize(nn.Module):\n",
    "    def __init__(self, embed_dim) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        return self.layer_norm(x + z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4eab6599",
   "metadata": {},
   "source": [
    "### Positionwise Feed-forward Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9e10be6",
   "metadata": {},
   "source": [
    "Both encoder and decoder contain a fully connected feed-forward network after the attention sub layers. For each position, similar linear transformations with a ReLU activation in between is performed.\n",
    "$$FFN(x) = max(0, xW1 + b1 )W2 + b2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56f5e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define PositionWise FFN class\n",
    "class PositionWiseFFN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, pf_dim ,dropout) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.w1 = nn.Linear(embed_dim, pf_dim)\n",
    "        self.w2 = nn.Linear(pf_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.w1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.w2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35a49064",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae2f4d8f",
   "metadata": {},
   "source": [
    "The encoder layer consists of $\\{multi-headAttention, addAndNorm, FFN, addAndNorm\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3838454",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/f3jpYWe.png\" alt= “” width=\"300px\" height=\"350px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d08d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Encoder Layer class\n",
    "import torch\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, pf_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multihead_attention = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.feedforward = PositionWiseFFN(embed_dim, pf_dim, dropout)\n",
    "        self.add_and_norm_1 = AddAndNormalize(embed_dim)\n",
    "        self.add_and_norm_2 = AddAndNormalize(embed_dim)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        # mask shape: (batch_size, 1, 1, seq_len)\n",
    "\n",
    "        z, _ = self.multihead_attention(x, x, x, mask)\n",
    "        x = self.add_and_norm_1(x, z)\n",
    "\n",
    "        z = self.feedforward(x)\n",
    "        x = self.add_and_norm_2(x, z)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "495bc81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Encoder Layer\n",
    "embed_dim = 8\n",
    "max_length = 3\n",
    "num_heads = 2\n",
    "dropout = 0.5\n",
    "pf_dim = 4\n",
    "bs = 2\n",
    "\n",
    "\n",
    "x = torch.rand(size=(bs, max_length, embed_dim))\n",
    "mask = torch.tril(torch.ones(size=(bs, 1, 1, max_length)))\n",
    "\n",
    "encoder_layer = EncoderLayer(embed_dim, num_heads, dropout, pf_dim)\n",
    "encoder_layer(x, mask).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c733255f",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36289c92",
   "metadata": {},
   "source": [
    "The Encoder of the transformers consist of N EncoderLayer, the positional encoding and the token embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5923df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EncoderTransformer layer\n",
    "class EncoderTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, pf_dim, vocab_size, max_seq_length, n_layers, device='cpu') -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, embed_dim, max_seq_length, device)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(embed_dim, num_heads, dropout, pf_dim) for _ in range(n_layers)])\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        # mask shape: (batch_size, 1, 1, seq_length)\n",
    "\n",
    "        x = self.embedding(x)  # x shape:  (batch_size, seq_length, embed_dim)\n",
    "\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, mask)  # x shape: (batch_size, seq_length, embed_dim)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31f279d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test EncoderTransformer\n",
    "embed_dim = 8\n",
    "max_length = 3\n",
    "num_heads = 2\n",
    "vocab_size = 10\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "pf_dim = 4\n",
    "bs = 2\n",
    "\n",
    "x = torch.randint(0, vocab_size-1, size=(bs, max_length))\n",
    "mask = ((x != 0)*1).unsqueeze(1).unsqueeze(2) # dummy mask\n",
    "\n",
    "encoder = EncoderTransformer(embed_dim, num_heads, dropout, pf_dim, vocab_size, max_length, n_layers)\n",
    "z = encoder(x, mask)\n",
    "z.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0341144f",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14bd27ac",
   "metadata": {},
   "source": [
    "The decoder block in the transformer consists of n blocks of ${maskedMul-tiheadAttention, addAndNorm, encoderDecoderAttention, addAndNorm, FFN, addAndNorm}$\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/HJfpj2Y.png\" alt= “” width=\"300px\" height=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cacfc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define decoder layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_attention = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.encoder_decoder_attention = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.feedforward = PositionWiseFFN(embed_dim, pf_dim, dropout)\n",
    "\n",
    "        self.add_and_norm_1 = AddAndNormalize(embed_dim)\n",
    "        self.add_and_norm_2 = AddAndNormalize(embed_dim)\n",
    "        self.add_and_norm_3 = AddAndNormalize(embed_dim)\n",
    "\n",
    "    def forward(self, x, encoder_output, encoder_mask, decoder_mask):\n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        # encoder_output shape: (batch_size, seq_len, embed_dim)\n",
    "        # encoder_mask shape: (batch_size, 1, 1, seq_len)\n",
    "        # decoder_mask shape: (batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "        z, _ = self.mask_attention(x, x, x, mask=decoder_mask)\n",
    "        x = self.add_and_norm_1(x, z)\n",
    "\n",
    "        z, attention = self.encoder_decoder_attention(q=x, k=encoder_output, v=encoder_output, mask=encoder_mask)\n",
    "        x = self.add_and_norm_2(x, z)\n",
    "\n",
    "        z = self.feedforward(x)\n",
    "        x = self.add_and_norm_3(x, z)\n",
    "\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d43e2348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 8])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the DecoderLayer\n",
    "embed_dim = 8\n",
    "max_length = 5\n",
    "num_heads = 2\n",
    "dropout = 0.5\n",
    "pf_dim = 4\n",
    "vocab_size = 10\n",
    "bs = 2\n",
    "\n",
    "\n",
    "\n",
    "x = torch.randint(0, vocab_size, size=(bs, max_length))\n",
    "encoder_mask = ((x != 0)*1).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "x_embeddings = torch.rand(size=(bs, max_length, embed_dim))\n",
    "encoder_output = torch.rand(size=(bs, max_length, embed_dim))\n",
    "decoder_mask = torch.tril(torch.ones(size=(bs, 1, max_length, max_length)))\n",
    "\n",
    "\n",
    "\n",
    "decoder_layer = DecoderLayer(embed_dim, num_heads, pf_dim, dropout)\n",
    "z, attention = decoder_layer(x_embeddings, encoder_output, encoder_mask, decoder_mask)\n",
    "z.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bc87652",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d929aea",
   "metadata": {},
   "source": [
    "The Decoder of a transformer consist of N DecoderLayer, the positional encoding and the token embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f9b576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DecoderTransformer layer\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, pf_dim, vocab_size, max_seq_length, n_layers, device=\"cpu\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, embed_dim, max_seq_length, device)\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(embed_dim, num_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, encoder_output, encoder_mask, decoder_mask):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        for layer in self.decoder:\n",
    "            x, _ = layer(x, encoder_output, encoder_mask, decoder_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f87c77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# Test the DecoderTransformer layer\n",
    "embed_dim = 8\n",
    "max_length = 5\n",
    "num_heads = 2\n",
    "dropout = 0.5\n",
    "pf_dim = 4\n",
    "vocab_size = 10\n",
    "bs = 2\n",
    "n_layers = 2\n",
    "\n",
    "x = torch.randint(0, vocab_size, size=(bs, max_length))\n",
    "encoder_output = torch.rand(size=(bs, max_length, embed_dim))\n",
    "decoder_mask = torch.tril(torch.ones(size=(bs, 1, max_length, max_length)))\n",
    "encoder_mask = ((x != 0)*1).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "decoder = DecoderTransformer(embed_dim, num_heads, dropout, pf_dim, vocab_size, max_length, n_layers)\n",
    "z = decoder(x, encoder_output, encoder_mask, decoder_mask)\n",
    "print(z.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbf97c03",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98c99a8b",
   "metadata": {},
   "source": [
    "The transformer model is composed of an EncoderTransformer and a DecoderTransformer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39f0059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        dropout,\n",
    "        pf_dim,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        max_seq_len,\n",
    "        src_lang,\n",
    "        trg_lang,\n",
    "        n_layers=6,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.src_lang = src_lang\n",
    "        self.trg_lang = trg_lang\n",
    "\n",
    "        self.encoder = EncoderTransformer(embed_dim, num_heads, dropout, pf_dim, src_vocab_size, max_seq_len, n_layers, device)\n",
    "        self.decoder = DecoderTransformer(embed_dim, num_heads, dropout, pf_dim, trg_vocab_size, max_seq_len, n_layers, device)\n",
    "        self.final_layer = nn.Linear(embed_dim, trg_vocab_size, device=device)\n",
    "\n",
    "        self.loss_fc = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=2e-5)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x shape: (batch_size, src_len)\n",
    "        # y shape: (batch_size, trg_len)\n",
    "\n",
    "        encoder_mask, decoder_mask = self.compute_masks(x, y)\n",
    "\n",
    "        encoder_output = self.encoder(x, encoder_mask)\n",
    "        decoder_output = self.decoder(y, encoder_output, encoder_mask, decoder_mask)\n",
    "\n",
    "        output = self.final_layer(decoder_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_masks(self, x, y):\n",
    "        encoder_mask = (x != 0).unsqueeze(1).unsqueeze(2)\n",
    "        decoder_mask = (y != 0).unsqueeze(1).unsqueeze(2)\n",
    "        decoder_mask = decoder_mask & torch.tril(torch.ones(y.shape[0], 1, y.shape[1], y.shape[1], device=self.device)).bool()\n",
    "\n",
    "        encoder_mask = encoder_mask.to(self.device)\n",
    "        decoder_mask = decoder_mask.to(self.device)\n",
    "\n",
    "        return encoder_mask, decoder_mask\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        running_loss = 0.0\n",
    "        running_bleu = 0.0\n",
    "\n",
    "        self.train()\n",
    "        bar = tqdm(train_loader)\n",
    "\n",
    "        for step, (x, y_input, y_output) in enumerate(bar, 1):\n",
    "            x = x.to(self.device)  # x shape: (batch_size, src_len)\n",
    "            y_input = y_input.to(self.device)  # y_input shape: (batch_size, trg_len)\n",
    "            y_output = y_output.to(self.device)  # y_output shape: (batch_size, trg_len)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            output = self(x, y_input)  # output shape: (batch_size, trg_len, trg_vocab_size)\n",
    "\n",
    "            loss = self.loss_fc(output.reshape(-1, output.shape[2]), y_output.reshape(-1))\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
    "\n",
    "            self.optimizer.step()\n",
    " \n",
    "            output = output.argmax(dim=-1).cpu().numpy()\n",
    "            y_output = y_output.cpu().numpy()\n",
    "\n",
    "            prediction = np.array([self.trg_lang.inverse_transform(pred) for pred in output])\n",
    "            y_output = np.array([self.trg_lang.inverse_transform(y_out) for y_out in y_output])\n",
    "\n",
    "            running_bleu += bleu_score(preds=prediction, target=y_output)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            bar.set_description(f\"Training Step: {step}, Training Loss: {running_loss / step:.4f}, Training BLEU: {running_bleu / step:.4f}\")\n",
    "\n",
    "        logs = {\"Training Loss\": running_loss / step, \"Training BLEU\": running_bleu / step}\n",
    "\n",
    "        return logs\n",
    "\n",
    "    def test_one_epoch(self, test_loader):\n",
    "        running_loss = 0.0\n",
    "        running_bleu = 0.0\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        bar = tqdm(test_loader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for step, (x, y_input, y_output) in enumerate(bar, 1):\n",
    "                x = x.to(self.device)\n",
    "                y_input = y_input.to(self.device)\n",
    "                y_output = y_output.to(self.device)\n",
    "\n",
    "                output = self(x, y_input)\n",
    "\n",
    "                loss = self.loss_fc(output.reshape(-1, output.shape[2]), y_output.reshape(-1))\n",
    "\n",
    "                output = output.argmax(dim=-1).cpu().numpy()\n",
    "                y_output = y_output.cpu().numpy()\n",
    "\n",
    "                prediction = np.array([self.trg_lang.inverse_transform(pred) for pred in output])\n",
    "                y_output = np.array([self.trg_lang.inverse_transform(y_out) for y_out in y_output])\n",
    "\n",
    "                running_bleu += bleu_score(preds=prediction, target=y_output)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                bar.set_description(f\"Test Step: {step}, Test Loss: {running_loss / step:.4f}, Test BLEU: {running_bleu / step:.4f}\")\n",
    "\n",
    "        logs = {\"Test Loss\": running_loss / step, \"Test BLEU\": running_bleu / step}\n",
    "\n",
    "        return logs\n",
    "\n",
    "\n",
    "    def fit(self, train_loader, test_loader, epochs):\n",
    "        bar = tqdm(range(epochs))\n",
    "\n",
    "        for epoch in bar:\n",
    "            logs_train = self.train_one_epoch(train_loader)\n",
    "            logs_test = self.test_one_epoch(test_loader)\n",
    "\n",
    "            bar.set_description(f\"Epoch: {epoch}\")\n",
    "\n",
    "    def translate_sentence(self, sentence):\n",
    "        src_sentence = torch.Tensor(self.src_lang.transform(sentence, start_token=True, end_token=True)).long().to(self.device)\n",
    "        src_sentence = src_sentence.unsqueeze(0)\n",
    "\n",
    "        trg_sentence = torch.Tensor(np.zeros(shape=(1, self.max_seq_len))).long().to(self.device)\n",
    "        trg_sentence[0, 0] = self.trg_lang.word2index[\"<start>\"]\n",
    "\n",
    "        encoder_mask, decoder_mask = self.compute_masks(src_sentence, trg_sentence)\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoder_output = self.encoder(src_sentence, encoder_mask)\n",
    "\n",
    "            for i in range(1, self.max_seq_len):\n",
    "                decoder_output = self.decoder(trg_sentence, encoder_output, encoder_mask, decoder_mask)\n",
    "                output = self.final_layer(decoder_output)\n",
    "                output = output.squeeze(0).argmax(dim=-1)\n",
    "                trg_sentence[0, i] = output[i - 1]\n",
    "\n",
    "                if output[i - 1] == self.trg_lang.word2index[\"<end>\"]:\n",
    "                    break\n",
    "\n",
    "                encoder_mask, decoder_mask = self.compute_masks(src_sentence, trg_sentence)\n",
    "\n",
    "        output = output.cpu().numpy().tolist()\n",
    "\n",
    "        output = self.trg_lang.inverse_transform(output[:i])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9df78f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46806f05c9514ca0918ec0bbbf8330e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5325cf11874061800476d7349aebdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train test split\n",
    "from torch.utils.data import DataLoader\n",
    "dataset_train, dataset_test = train_test_split(dataset.sample(50000), test_size=0.2, random_state=42)\n",
    "\n",
    "# pytorch datasets\n",
    "ds_train = CustomDataset(dataset_train, max_seq_len=100)\n",
    "ds_test = CustomDataset(dataset_test, max_seq_len=100, src_lang=ds_train.src_lang, trg_lang=ds_train.trg_lang)\n",
    "\n",
    "#Dataloader\n",
    "loader_train = DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=5, prefetch_factor=5)\n",
    "loader_test = DataLoader(ds_test, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e024d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance model\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "dropout = 0.3\n",
    "pf_dim = 512\n",
    "src_vocab_size = ds_train.src_lang.n_words\n",
    "trg_vocab_size = ds_train.trg_lang.n_words\n",
    "max_length = ds_train.max_seq_len\n",
    "n_layers = 6\n",
    "device = \"cuda\"\n",
    "\n",
    "transformer = Transformer(\n",
    "    embed_dim,\n",
    "    num_heads,\n",
    "    dropout,\n",
    "    pf_dim,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    max_length,\n",
    "    ds_train.src_lang,\n",
    "    ds_train.trg_lang,\n",
    "    n_layers,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "978d7cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fd4c2a9bb44dca978acf43d3898fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8473815a554449186f932f1ece68431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/.pyenv/versions/3.10.12/envs/NLP/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:70: FutureWarning: Importing `bleu_score` from `torchmetrics.functional` was deprecated and will be removed in 2.0. Import `bleu_score` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11660897805f487ea3d9dc0c937f6db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d99f9e6cd04085b457f2495cd062fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f00a62fff1494a9875baa2482f954c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deaac3f840644098859d559df96ca4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc12f6c851f54ecc8c8adabd10f4205f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c140d0833142b48d3a53e58dbcd7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa4c3a4a9f8444387eed715e38c5fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8e25aefe2e4d12b29343e6fb418b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474e61e276694faca08eda7b14870680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b1b2af65194d0f9b43672f59bf3c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d027d351e14e609eb8012a9d463c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e03013565a14b908352e4924e9a50ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04548b420a314689bc645071779f8bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b31382e76540bcb49d62c710cc10b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed16d9ffe974c9f87f5716385eecb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd844f1620e44b7baa26449c9c995422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2370586f18f24ba88a8da1df831606ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856343bc923040e78f5f67975592ead9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5323a7c10a1a423dbb7b3c5d2e7a5d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fit the model\n",
    "transformer.fit(loader_train, loader_test, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de2a0d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'estoy realmente feliz <end>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# short translation\n",
    "transformer.translate_sentence(\"i am really happy here!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0435c29a728a8af6f6141793e2794fa2251761d1c1dcca19e68ebfa0c489a176"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
