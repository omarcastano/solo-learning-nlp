{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c08a40b",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "This notebook show a basic implementation of BERT pre-training using the wikipedia dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# black formatting with jupyter-black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    lab=True,\n",
    "    line_length=140,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f297020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaries\n",
    "import torch\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import multiprocessing as mp\n",
    "\n",
    "from typing import List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import re\n",
    "\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "from utils import text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55811346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download wikipedia dataset\n",
    "data = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[0:500]\", trust_remote_code=True).to_pandas()\n",
    "# data = load_dataset(\"karpathy/tiny_shakespeare\", split=\"train\", trust_remote_code=True).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a723f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each text by \".\"\n",
    "texts = []\n",
    "\n",
    "for t_id, t in enumerate(data.text.tolist()):\n",
    "    t = t.split(\".\")\n",
    "    for sentence_id, sentence in enumerate(t):\n",
    "        texts.append({\"paragraph_id\": t_id, \"sentence_id\": sentence_id, \"text\": sentence})\n",
    "\n",
    "data = pd.DataFrame(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd54d8f",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf27cc",
   "metadata": {},
   "source": [
    "We have implemented a Byte-Per Encoding Tokenizer. However, this python implementation is really slow and so we will use a transformers implementation of Word Piece Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eacd820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import BertPreTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Set the pre-tokenizer to a custom one\n",
    "tokenizer.pre_tokenizer = BertPreTokenizer()\n",
    "\n",
    "# Enable padding for the tokenizer\n",
    "# tokenizer.enable_padding()\n",
    "\n",
    "# Initialize a trainer with desired parameters\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=30000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "\n",
    "# preprocess data\n",
    "data.text = data.text.apply(text_preprocessing)\n",
    "\n",
    "# Load your training data into a list of strings\n",
    "train_data = data.text.tolist()\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(\n",
    "    train_data,\n",
    "    trainer=trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e6222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"praying, she's a good person\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deff109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of words\n",
    "data[\"text_length\"] = data.text.apply(lambda x: len(tokenizer.encode(x).tokens))\n",
    "data = data.query(\"text_length>=5 and text_length<=60\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of words distribution\n",
    "sns.histplot(data=data, x=\"text_length\", kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd990621",
   "metadata": {},
   "source": [
    "## Create a dataset with  positive and negative samples for the Next Sentence Prediction (NSP) task\n",
    "\n",
    "Now we create the dataset that will be sued for Next Sentence Prediction. In NSP, the model is given two sentences and must predict whether the second sentence logically follows the first sentence in a coherent narrative or if the two sentences are unrelated. This task helps the model understand the relationships between sentences and improve its comprehension of text flow, context, and continuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c4359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nsp_sample(dataset, idx):\n",
    "    \"\"\"\n",
    "    Generates a sample for the Next Sentence Prediction (NSP) task.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        dataset (pd.DataFrame): The dataset containing text, paragraph_id, and sentence_id columns.\n",
    "        idx (int): The index of the sentence to be used as the first sentence (sentence_a).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        tuple: A tuple containing:\n",
    "            - sentence_a (str): The first sentence.\n",
    "            - sentence_b (str): The second sentence, which can either be a coherent follow-up sentence or an unrelated one.\n",
    "            - nsp_label (int): The label indicating if sentence_b is a follow-up (1) or not (0).\n",
    "    \"\"\"\n",
    "    sentence_a = dataset.loc[idx, \"text\"]\n",
    "    paragraph_id_a = dataset.loc[idx, \"paragraph_id\"]\n",
    "    sentence_id_a = dataset.loc[idx, \"sentence_id\"]\n",
    "\n",
    "    prob_nsp = np.random.random()\n",
    "\n",
    "    if prob_nsp >= 0.5:\n",
    "        sentence_b, nsp_label = get_positive_pair(dataset, paragraph_id_a, sentence_id_a)\n",
    "    else:\n",
    "        sentence_b, nsp_label = get_negative_pair(dataset, paragraph_id_a, sentence_id_a, hard_negative=False)\n",
    "\n",
    "    if sentence_b is None:\n",
    "        sentence_b, nsp_label = get_negative_pair(dataset, paragraph_id_a, sentence_id_a, hard_negative=False)\n",
    "\n",
    "    return sentence_a, sentence_b, nsp_label\n",
    "\n",
    "\n",
    "def get_positive_pair(dataset, paragraph_id_a, sentence_id_a):\n",
    "    \"\"\"\n",
    "    Retrieves a positive sentence pair for the NSP task, where the second sentence follows the first.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        dataset (pd.DataFrame): The dataset containing text, paragraph_id, and sentence_id columns.\n",
    "        paragraph_id_a (int): The paragraph ID of the first sentence.\n",
    "        sentence_id_a (int): The sentence ID of the first sentence.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        tuple: A tuple containing:\n",
    "            - sentence_b (str or None): The next sentence if it exists, otherwise None.\n",
    "            - nsp_label (int): The label indicating this is a follow-up sentence (1).\n",
    "    \"\"\"\n",
    "    nsp_label = 1\n",
    "    sentence_id_b = sentence_id_a + 1\n",
    "\n",
    "    try:\n",
    "        sentence_b = dataset.query(f\"paragraph_id == {paragraph_id_a} and sentence_id == {sentence_id_b}\").text.iloc[0]\n",
    "    except:\n",
    "        sentence_b = None\n",
    "\n",
    "    return sentence_b, nsp_label\n",
    "\n",
    "\n",
    "def get_negative_pair(dataset, paragraph_id_a, sentence_id_a, hard_negative=False):\n",
    "    \"\"\"\n",
    "    Retrieves a negative sentence pair for the NSP task, where the second sentence does not follow the first.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        dataset (pd.DataFrame): The dataset containing text, paragraph_id, and sentence_id columns.\n",
    "        paragraph_id_a (int): The paragraph ID of the first sentence.\n",
    "        sentence_id_a (int): The sentence ID of the first sentence.\n",
    "        hard_negative (bool): If True, the second sentence is chosen from the same paragraph but is not the next sentence.\n",
    "                              If False, the second sentence is chosen from a different paragraph.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        tuple: A tuple containing:\n",
    "            - sentence_b (str): The unrelated sentence.\n",
    "            - nsp_label (int): The label indicating this is not a follow-up sentence (0).\n",
    "    \"\"\"\n",
    "    nsp_label = 0\n",
    "\n",
    "    if hard_negative:\n",
    "        sentence_b = dataset.query(f\"paragraph_id == {paragraph_id_a} and sentence_id != {sentence_id_a}\").sample(1).text.iloc[0]\n",
    "    else:\n",
    "        sentence_b = dataset.query(f\"paragraph_id != {paragraph_id_a}\").sample(1).text.iloc[0]\n",
    "\n",
    "    return sentence_b, nsp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba38a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset for NSP\n",
    "sentence_a = []\n",
    "sentence_b = []\n",
    "label = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "\n",
    "    a, b, l = get_nsp_sample(data, i)\n",
    "\n",
    "    sentence_a.append(a)\n",
    "    sentence_b.append(b)\n",
    "    label.append(l)\n",
    "\n",
    "data[\"sentence_a\"] = sentence_a\n",
    "data[\"sentence_b\"] = sentence_b\n",
    "data[\"label\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b51888",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd227107",
   "metadata": {},
   "source": [
    "# Custom Dataset And Data Collator\n",
    "\n",
    "We define a custom data that prepare data MLM and NSP pre-trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30cc820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for handling sentence pairs and generating masked language model (MLM) and\n",
    "    next sentence prediction (NSP) labels.\n",
    "\n",
    "    Attributes:\n",
    "        dataset (pd.DataFrame): The dataset containing sentence pairs and labels.\n",
    "        hard_negative (bool): A flag to indicate if hard negative sampling is used.\n",
    "        sep_token_id (list): Token ID for the [SEP] token.\n",
    "        cls_token_id (list): Token ID for the [CLS] token.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: pd.DataFrame, hard_negative=False) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the CustomDataset with the provided dataset and optional hard negative sampling flag.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            dataset (pd.DataFrame): The input dataset containing sentence pairs and labels.\n",
    "            hard_negative (bool): A flag indicating whether to use hard negative sampling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.hard_negative = hard_negative\n",
    "        self.sep_token_id = tokenizer.encode(\"[SEP]\").ids\n",
    "        self.cls_token_id = tokenizer.encode(\"[CLS]\").ids\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the sample at the specified index and processes it to generate input IDs, MLM labels,\n",
    "        and NSP label.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the input IDs, MLM labels, and NSP label.\n",
    "        \"\"\"\n",
    "        # Retrieve sentence pairs and NSP label from the dataset\n",
    "        sentence_a = self.dataset.sentence_a.tolist()[idx]\n",
    "        sentence_b = self.dataset.sentence_b.tolist()[idx]\n",
    "        nsp_label = self.dataset.label.tolist()[idx]\n",
    "\n",
    "        # Construct the input sentence with special tokens\n",
    "        sentence = f\"[CLS] {sentence_a} [SEP] {sentence_b}\"\n",
    "        ids, mlm_labels = self.get_masked_sentence(sentence)\n",
    "\n",
    "        return ids, mlm_labels, nsp_label\n",
    "\n",
    "    def get_masked_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Applies masking to the input sentence to generate MLM labels.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            sentence (str): The input sentence with special tokens.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the masked input IDs and the original input IDs as MLM labels.\n",
    "        \"\"\"\n",
    "        # Encode the sentence into token IDs\n",
    "        encoded_sentence = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        ids = np.array(encoded_sentence.ids)\n",
    "\n",
    "        # Determine the number of tokens to mask (15% of the total tokens)\n",
    "        n_mask_tokens = max(1, round(len(ids) * 0.15))\n",
    "\n",
    "        # Identify the index of the [SEP] token\n",
    "        sep_index = (ids == tokenizer.token_to_id(\"[SEP]\")).argmax()\n",
    "\n",
    "        # Create a list of candidate tokens for masking, excluding special tokens\n",
    "        candidate_mask = np.arange(len(ids))\n",
    "        candidate_mask = candidate_mask[~np.isin(candidate_mask, [0, sep_index])]\n",
    "\n",
    "        # Randomly select tokens to mask\n",
    "        selected_tokens = np.random.choice(candidate_mask, size=n_mask_tokens, replace=False)\n",
    "        mlm_labels = ids.copy()\n",
    "\n",
    "        # Replace selected tokens with the [MASK] token ID\n",
    "        ids[selected_tokens] = tokenizer.token_to_id(\"[MASK]\")\n",
    "\n",
    "        return ids.tolist(), mlm_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8707ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for handling sentence pairs and generating masked language model (MLM) and\n",
    "    next sentence prediction (NSP) labels.\n",
    "\n",
    "    Attributes:\n",
    "        dataset (pd.DataFrame): The dataset containing sentence pairs and labels.\n",
    "        hard_negative (bool): A flag to indicate if hard negative sampling is used.\n",
    "        sep_token_id (list): Token ID for the [SEP] token.\n",
    "        cls_token_id (list): Token ID for the [CLS] token.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: pd.DataFrame, hard_negative=False) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the CustomDataset with the provided dataset and optional hard negative sampling flag.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            dataset (pd.DataFrame): The input dataset containing sentence pairs and labels.\n",
    "            hard_negative (bool): A flag indicating whether to use hard negative sampling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.hard_negative = hard_negative\n",
    "        self.sep_token_id = tokenizer.encode(\"[SEP]\").ids\n",
    "        self.cls_token_id = tokenizer.encode(\"[CLS]\").ids\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the sample at the specified index and processes it to generate input IDs, MLM labels,\n",
    "        and NSP label.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the input IDs, MLM labels, and NSP label.\n",
    "        \"\"\"\n",
    "        # Retrieve sentence pairs and NSP label from the dataset\n",
    "        sentence_a = self.dataset.sentence_a.tolist()[idx]\n",
    "        sentence_b = self.dataset.sentence_b.tolist()[idx]\n",
    "        nsp_label = self.dataset.label.tolist()[idx]\n",
    "\n",
    "        # Construct the input sentence with special tokens\n",
    "        sentence = f\"[CLS] {sentence_a} [SEP] {sentence_b}\"\n",
    "        ids, mlm_labels = self.get_masked_sentence(sentence)\n",
    "\n",
    "        return ids, mlm_labels, nsp_label\n",
    "\n",
    "    def get_masked_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Applies masking to the input sentence to generate MLM labels.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            sentence (str): The input sentence with special tokens.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the masked input IDs and the original input IDs as MLM labels.\n",
    "        \"\"\"\n",
    "        # Encode the sentence into token IDs\n",
    "        encoded_sentence = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        ids = np.array(encoded_sentence.ids)\n",
    "\n",
    "        # Determine the number of tokens to mask (15% of the total tokens)\n",
    "        n_mask_tokens = max(1, round(len(ids) * 0.15))\n",
    "\n",
    "        # Identify the index of the [SEP] token\n",
    "        sep_index = (ids == tokenizer.token_to_id(\"[SEP]\")).argmax()\n",
    "\n",
    "        # Create a list of candidate tokens for masking, excluding special tokens\n",
    "        candidate_mask = np.arange(len(ids))\n",
    "        candidate_mask = candidate_mask[~np.isin(candidate_mask, [0, sep_index])]\n",
    "\n",
    "        # Randomly select tokens to mask\n",
    "        selected_tokens = np.random.choice(candidate_mask, size=n_mask_tokens, replace=False)\n",
    "        mlm_labels = ids.copy()\n",
    "\n",
    "        # Replace selected tokens with the [MASK] token ID\n",
    "        ids[selected_tokens] = tokenizer.token_to_id(\"[MASK]\")\n",
    "\n",
    "        return ids.tolist(), mlm_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d695c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorForMLMAndNSP:\n",
    "    \"\"\"\n",
    "    A data collator class for preparing batches of data for Masked Language Modeling (MLM)\n",
    "    and Next Sentence Prediction (NSP).\n",
    "\n",
    "    Attributes:\n",
    "        pad_token_id (int): The token ID used for padding sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pad_token_id=0) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the DataCollatorForMLMAndNSP with the specified padding token ID.\n",
    "\n",
    "        Args:\n",
    "            pad_token_id (int): The token ID to use for padding sequences. Default is 0.\n",
    "        \"\"\"\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def __call__(self, sentences):\n",
    "        \"\"\"\n",
    "        Processes a batch of sentences to generate padded token IDs, attention masks, MLM labels,\n",
    "        and NSP labels.\n",
    "\n",
    "        Args:\n",
    "            sentences (list of tuples): A list where each tuple contains token IDs, MLM labels,\n",
    "                                        and an NSP label.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the padded token IDs, attention masks, MLM labels, and NSP labels.\n",
    "        \"\"\"\n",
    "        # Convert lists of token IDs and MLM labels to tensors\n",
    "        ids = [torch.LongTensor(tokens[0]) for tokens in sentences]\n",
    "        mlm_labels = [torch.LongTensor(tokens[1]) for tokens in sentences]\n",
    "        nsp_labels = torch.LongTensor([[tokens[2]] for tokens in sentences]).squeeze(1)\n",
    "\n",
    "        # Pad the sequences to have the same length\n",
    "        token_ids = pad_sequence(ids, padding_value=self.pad_token_id, batch_first=True)\n",
    "        mlm_labels = pad_sequence(mlm_labels, padding_value=self.pad_token_id, batch_first=True)\n",
    "\n",
    "        # Create attention masks (1 for real tokens, 0 for padding tokens)\n",
    "        attention_masks = torch.ne(token_ids, self.pad_token_id).long()\n",
    "        attention_masks = attention_masks.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        return token_ids, attention_masks, mlm_labels, nsp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b6ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CustomDataset(data, False)\n",
    "data_loader = DataLoader(ds, batch_size=2, collate_fn=DataCollatorForMLMAndNSP(pad_token_id=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4adf4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, attention_mask, mlm_labels, nsp_labels = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8bf8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f862c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5117a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e35973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokens[0].tolist(), skip_special_tokens=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "469d9c4d",
   "metadata": {},
   "source": [
    "## BERT Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import EncoderTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A BERT model class for handling masked language modeling (MLM) and next sentence prediction (NSP).\n",
    "\n",
    "    Attributes:\n",
    "        encoder (EncoderTransformer): The encoder transformer network.\n",
    "        nsp_dnn (torch.nn.Linear): The linear layer for NSP.\n",
    "        mlm_dnn (torch.nn.Linear): The linear layer for MLM.\n",
    "        device (str): The device to run the model on.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout, pf_dim, vocab_size, max_length, n_layers, device) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the BERT model with the specified parameters.\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): The embedding dimension.\n",
    "            num_heads (int): The number of attention heads.\n",
    "            dropout (float): The dropout rate.\n",
    "            pf_dim (int): The position-wise feed-forward dimension.\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "            max_length (int): The maximum sequence length.\n",
    "            n_layers (int): The number of layers in the transformer.\n",
    "            device (str): The device to run the model on.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = EncoderTransformer(embed_dim, num_heads, dropout, pf_dim, vocab_size, max_length, n_layers, device=device)\n",
    "        self.nsp_dnn = torch.nn.Linear(embed_dim, 2, device=device)\n",
    "        self.mlm_dnn = torch.nn.Linear(embed_dim, vocab_size, device=device)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def config_training_args(self, optimizer, optimizer_kwargs={}) -> None:\n",
    "        \"\"\"\n",
    "        Configures the optimizer for training.\n",
    "\n",
    "        Args:\n",
    "            optimizer (torch.optim.Optimizer): The optimizer class.\n",
    "            optimizer_kwargs (dict): Additional keyword arguments for the optimizer.\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer(self.parameters(), **optimizer_kwargs)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass through the BERT model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor containing token IDs.\n",
    "            mask (torch.Tensor): The attention mask tensor.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the MLM logits and NSP logits.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        # Synchronize CUDA\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Pass through the encoder\n",
    "        x = self.encoder(x, mask)\n",
    "\n",
    "        # Compute MLM and NSP predictions\n",
    "        y_mlm = self.mlm_dnn(x)\n",
    "        y_nsp = self.nsp_dnn(x[:, 0, :])  # Only use the [CLS] token for NSP\n",
    "\n",
    "        return y_mlm, y_nsp\n",
    "\n",
    "    def train_one_epoch(self, train_loader) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model for one epoch.\n",
    "\n",
    "        Args:\n",
    "            train_loader (DataLoader): The DataLoader for the training data.\n",
    "        \"\"\"\n",
    "        bar = tqdm(train_loader)\n",
    "        running_total_loss = 0\n",
    "        running_mlm_loss = 0\n",
    "        running_nsp_loss = 0\n",
    "        running_f1 = 0\n",
    "\n",
    "        for step, (ids, attention_mask, mlm_labels, nsp_labels) in enumerate(bar, 1):\n",
    "\n",
    "            # Map to device\n",
    "            ids = ids.to(self.device)\n",
    "            attention_mask = attention_mask.to(self.device)\n",
    "            mlm_labels = mlm_labels.to(self.device)\n",
    "            nsp_labels = nsp_labels.to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_mlm, y_nsp = self(ids, attention_mask)\n",
    "            y_mlm = y_mlm.reshape(-1, y_mlm.shape[-1])\n",
    "            mlm_labels = mlm_labels.reshape(-1)\n",
    "\n",
    "            # Compute MLM loss\n",
    "            mlm_loss = torch.nn.functional.cross_entropy(y_mlm, mlm_labels, ignore_index=0)\n",
    "\n",
    "            # Compute NSP loss\n",
    "            nsp_loss = torch.nn.functional.cross_entropy(y_nsp, nsp_labels)\n",
    "\n",
    "            # Total loss\n",
    "            loss = nsp_loss + mlm_loss\n",
    "\n",
    "            # Clear gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
    "\n",
    "            # Optimization step\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Running total losses\n",
    "            running_mlm_loss += mlm_loss.item()\n",
    "            running_nsp_loss += nsp_loss.item()\n",
    "            running_total_loss += loss.item()\n",
    "\n",
    "            # Running F1 score\n",
    "            y_nsp = torch.argmax(y_nsp, dim=-1)\n",
    "            running_f1 += f1_score(nsp_labels.cpu(), y_nsp.cpu(), average=\"macro\")\n",
    "\n",
    "            # Update progress bar description\n",
    "            bar.set_description(\n",
    "                f\"total loss: {running_total_loss / step:0.3f} | \"\n",
    "                + f\"mlm loss: {running_mlm_loss / step:0.3f} | \"\n",
    "                + f\"nsp loss: {running_nsp_loss / step:0.3f} | \"\n",
    "                + f\"f1: {running_f1 / step:0.3f}\"\n",
    "            )\n",
    "\n",
    "    def train(self, train_data, num_epochs) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model for a specified number of epochs.\n",
    "\n",
    "        Args:\n",
    "            train_data (DataLoader): The DataLoader for the training data.\n",
    "            num_epochs (int): The number of epochs to train for.\n",
    "        \"\"\"\n",
    "        bar = tqdm(range(num_epochs))\n",
    "\n",
    "        for epoch in bar:\n",
    "            self.train_one_epoch(train_data)\n",
    "            bar.set_description(f\"epoch: {epoch}\")\n",
    "\n",
    "    def mask_filling(self, sentence: str, tokenizer) -> str:\n",
    "        \"\"\"\n",
    "        Fills masked tokens in a given sentence.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): The input sentence with masked tokens.\n",
    "            tokenizer: The tokenizer used to encode and decode the sentence.\n",
    "\n",
    "        Returns:\n",
    "            str: The sentence with masked tokens filled.\n",
    "        \"\"\"\n",
    "        sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "        token_ids = torch.LongTensor([tokenizer.encode(sentence).ids]).to(self.device)\n",
    "        attention_mask = torch.ne(token_ids, tokenizer.encode(\"[PAD]\").ids[0]).long().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_mlm, _ = self(token_ids, attention_mask)\n",
    "            y_mlm = y_mlm.argmax(dim=-1)\n",
    "            sentence = tokenizer.decode(y_mlm[0].tolist())\n",
    "\n",
    "            return sentence\n",
    "\n",
    "    def nsp_prediction(self, sentence_a: str, sentence_b: str, tokenizer) -> int:\n",
    "        \"\"\"\n",
    "        Predicts whether the second sentence is the next sentence of the first.\n",
    "\n",
    "        Args:\n",
    "            sentence_a (str): The first sentence.\n",
    "            sentence_b (str): The second sentence.\n",
    "            tokenizer: The tokenizer used to encode the sentences.\n",
    "\n",
    "        Returns:\n",
    "            int: The NSP prediction (0 or 1).\n",
    "        \"\"\"\n",
    "        sentence = \"[CLS] \" + sentence_a + \" [SEP] \" + sentence_b\n",
    "        token_ids = torch.LongTensor([tokenizer.encode(sentence).ids]).to(self.device)\n",
    "        attention_mask = torch.ne(token_ids, tokenizer.encode(\"[PAD]\").ids[0]).long().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, y_nsp = self(token_ids, attention_mask)\n",
    "            y_nsp = torch.argmax(y_nsp, dim=-1)\n",
    "\n",
    "        return y_nsp.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec63f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CustomDataset(data, False)\n",
    "data_loader = DataLoader(ds, batch_size=32, collate_fn=DataCollatorForMLMAndNSP(pad_token_id=0), num_workers=5, prefetch_factor=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model parameters\n",
    "embed_dim = 768\n",
    "max_length = 128\n",
    "num_heads = 8\n",
    "vocab_size = 30000\n",
    "n_layers = 12\n",
    "dropout = 0.3\n",
    "pf_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35cb918",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERT(embed_dim, num_heads, dropout, pf_dim, vocab_size, max_length, n_layers, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config model\n",
    "optimizer = torch.optim.Adam\n",
    "\n",
    "bert_model.config_training_args(optimizer=optimizer, optimizer_kwargs={\"lr\": 2e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d88e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.train(data_loader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09853d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.mask_filling(\n",
    "    \"humans lived in societies without formal hierarchies long before the [MASK] of formal states, realms, or [MASK]\",\n",
    "    tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a9abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.nsp_prediction(\n",
    "    sentence_a=\"humans lived in societies without formal hierarchies long before the establishment of formal states, realms, or empires\",\n",
    "    sentence_b=\"domesticated almonds appear in the early bronze age (bc), such as the archaeological sites of numeira (jordan), or possibly earlier\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0435c29a728a8af6f6141793e2794fa2251761d1c1dcca19e68ebfa0c489a176"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
