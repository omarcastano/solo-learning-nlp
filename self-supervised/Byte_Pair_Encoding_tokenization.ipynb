{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding tokenization\n",
    "\n",
    "This notebook illustrates a implementation of Byte-Pair Encoding tokenization using python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# black formatting with jupyter-black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    lab=True,\n",
    "    line_length=140,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaries\n",
    "import re\n",
    "\n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from utils import text_preprocessing\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IMDB dataset from huggingface\n",
    "imdb = load_dataset(\"imdb\")[\"unsupervised\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "imdb.text = imdb.text.apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "\n",
    "    def __init__(self, text: List[str], special_tokens=[\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\"]) -> None:\n",
    "        \"\"\"\n",
    "        Implementation of BPE algorithm\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        text: List[str]\n",
    "            Text to tokenize. It should be a list of setences.\n",
    "        \"\"\"\n",
    "\n",
    "        self.special_tokens = special_tokens\n",
    "        text = \" \".join(text)\n",
    "        self.text = self.pre_tokenization(text)\n",
    "        self.vocabulary = list(set(text))\n",
    "        self.vocabulary.remove(\" \")\n",
    "        self.token_to_id = {token: id for id, token in enumerate(special_tokens)}\n",
    "\n",
    "    def pre_tokenization(self, text: str):\n",
    "        \"\"\"\n",
    "        Pre tokenization of the text. It will remove the special tokens and replace the spaces with a special character.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        text: str\n",
    "            Text to tokenize. It should be a list of setences.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        text: List[str]\n",
    "        \"\"\"\n",
    "\n",
    "        pattern = \"(\" + \"|\".join(self.special_tokens) + \"|\\W|\\s)\"\n",
    "\n",
    "        text = text.strip()\n",
    "        text = re.sub(\"\\s+\", \" Ã‘\", text)\n",
    "        text = re.split(pattern, text)\n",
    "        text = [char for char in text if char not in [\"\", \" \"]]\n",
    "\n",
    "        return text\n",
    "\n",
    "    def get_words_count(self):\n",
    "        \"\"\"\n",
    "        Get the words count of the text.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            words_count: defaultdict(int)\n",
    "        \"\"\"\n",
    "\n",
    "        words_count = defaultdict(int)\n",
    "\n",
    "        for word in self.text:\n",
    "            words_count[word] += 1\n",
    "\n",
    "        return words_count\n",
    "\n",
    "    def get_initial_corpus(self):\n",
    "        \"\"\"\n",
    "        Get the initial corpus of the text.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            corpus: List[Tuple[List[str], int]]\n",
    "        \"\"\"\n",
    "\n",
    "        corpus = []\n",
    "        words_count = self.get_words_count()\n",
    "\n",
    "        for word, freq in words_count.items():\n",
    "            corpus.append((list(word), freq))\n",
    "\n",
    "        return corpus\n",
    "\n",
    "    def get_bigram_freq(self, corpus):\n",
    "        \"\"\"\n",
    "        Get the bigram frequency of the corpus.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            corpus: List[Tuple[List[str], int]]\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            bi_grams: defaultdict(int)\n",
    "        \"\"\"\n",
    "\n",
    "        bi_grams = defaultdict(int)\n",
    "\n",
    "        for char, freq in corpus:\n",
    "            for i in range(len(char) - 1):\n",
    "                bi_grams[char[i], char[i + 1]] += freq\n",
    "\n",
    "        return bi_grams\n",
    "\n",
    "    def update_corpus_and_vocab(self, bi_grams, corpus):\n",
    "        \"\"\"\n",
    "        Update the corpus and vocabulary.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            bi_grams: defaultdict(int)\n",
    "            corpus: List[Tuple[List[str], int]]\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            new_corpus: List[Tuple[List[str], int]]\n",
    "        \"\"\"\n",
    "\n",
    "        max_bi_gram = \"\".join(max(bi_grams, key=bi_grams.get))\n",
    "        self.vocabulary.append(max_bi_gram)\n",
    "\n",
    "        new_corpus = []\n",
    "\n",
    "        for char, freq in corpus:\n",
    "\n",
    "            if max_bi_gram in \"\".join(char):\n",
    "                char = \"\".join(char)\n",
    "                char = char.replace(max_bi_gram, \" \" + max_bi_gram + \" \").split()\n",
    "\n",
    "            new_corpus.append((char, freq))\n",
    "\n",
    "        return new_corpus\n",
    "\n",
    "    def train(self, vocab_size: int = 10):\n",
    "        \"\"\"\n",
    "        Train the tokenizer.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            vocab_size: int\n",
    "                Size of the vocabulary.\n",
    "        \"\"\"\n",
    "\n",
    "        corpus = self.get_initial_corpus()\n",
    "\n",
    "        bar = tqdm(range(vocab_size))\n",
    "        for _ in bar:\n",
    "            bi_grams = self.get_bigram_freq(corpus)\n",
    "            corpus = self.update_corpus_and_vocab(bi_grams, corpus)\n",
    "\n",
    "        self.corpus = corpus\n",
    "\n",
    "        for id, token in enumerate(self.vocabulary, len(self.special_tokens)):\n",
    "            self.token_to_id[token] = id\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        \"\"\"\n",
    "        Tokenize the text.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            text: str\n",
    "                Text to tokenize.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            tokenized_text: List[str]\n",
    "        \"\"\"\n",
    "\n",
    "        text = self.pre_tokenization(text)\n",
    "        tokenized_text = []\n",
    "\n",
    "        for word in text:\n",
    "            if word not in self.special_tokens:\n",
    "                word = list(word)\n",
    "                for voc in self.vocabulary:\n",
    "                    i = 0\n",
    "                    while i < len(word) - 1:\n",
    "                        if word[i] + word[i + 1] == voc:\n",
    "                            word = word[:i] + [voc] + word[i + 2 :]\n",
    "                        else:\n",
    "                            i += 1\n",
    "            else:\n",
    "                word = [word]\n",
    "\n",
    "            tokenized_text += word\n",
    "\n",
    "        for i in range(len(tokenized_text)):\n",
    "            if (tokenized_text[i] not in self.vocabulary) and (tokenized_text[i] not in self.special_tokens):\n",
    "                tokenized_text[i] = \"<unk>\"\n",
    "\n",
    "        ids = [self.token_to_id[token] for token in tokenized_text]\n",
    "\n",
    "        return tokenized_text, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of the data for testing\n",
    "text = imdb.text.tolist()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807c863559f14bf38c2f222dd869fa93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# instance tokenizer\n",
    "tokenizer = BPETokenizer(text)\n",
    "\n",
    "# train tokenizer\n",
    "tokenizer.train(vocab_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<cls>', 'i', 'Ã‘c', 'a', 'n', 't', 'Ã‘c', 'o', 'm', 'p', 'a', 'r', 'e', 'Ã‘th', 'i', 's', 'Ã‘m', 'o', 'v', 'i', 'e', 'Ã‘w', 'i', 'th', 'Ã‘an', 'y', 'th', 'ing', 'Ã‘e', 'l', 's', 'e', '<unk>', '<sep>', 'Ã‘m', 'a', 'y', 'b', 'e', 'Ã‘e', 'x', 'c', 'e', 'p', 't', 'Ã‘the', 'Ã‘m', 'o', 'v', 'i', 'e', 'Ã‘l', 'e', 'o', 'n', '<unk>', '<mask>', 'Ã‘p', 'l', 'a', 'y', 'e', 'd']\n",
      "[0, 85, 98, 26, 55, 16, 98, 73, 38, 32, 26, 47, 10, 146, 85, 40, 96, 73, 30, 85, 10, 90, 85, 145, 138, 49, 145, 171, 108, 66, 40, 10, 2, 1, 96, 26, 49, 31, 10, 108, 75, 79, 10, 32, 16, 185, 96, 73, 30, 85, 10, 103, 10, 73, 55, 2, 4, 102, 66, 26, 49, 10, 67]\n"
     ]
    }
   ],
   "source": [
    "tokens, ids = tokenizer.tokenize(\"<cls>i cant compare this movie with anything else <sep> maybe except the movie leon <mask> played\")\n",
    "print(tokens)\n",
    "print(ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
