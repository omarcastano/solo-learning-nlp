{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Generative Pretrained Transformer-1 (mGPT-1)\n",
    "\n",
    "This notebook illustrates a basic implementation of the Generative Pretrained Transformer model using PyTorch. We follow a similar approach as described in the original [GPT-1 paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf). Further details about the model architecture will be provided in the Mini GPT Architecture section. The specific sections covered in this notebook are:\n",
    "\n",
    "1. Data Downloading and Normalization\n",
    "2. Training Custom Tokenizer\n",
    "3. Building Custom Dataset and Data Loader\n",
    "4. Mini GPT Architecture\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "One of the most well-known successes of transformer-based language models has been text generation with the GPT-2, GPT-3, and GPT-4 language models, which are built from stacks of transformer-decoder layers. GPT (Generative Pre-training) is a family of generative pre-training models that are part of the language modeling trend where a model is first trained on unsupervised data, in a task-agnostic fashion, and later fine-tuned for a specific task. The first model in this family, the eponymous [GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), pre-trains a stack of transformer decoder layers on a large body of unlabeled text and is then fine-tuned on labeled, task-specific data. GPT is an autoregressive model, which means it uses inputs from previous steps of a sequence to predict values later in the sequence.\n",
    "\n",
    "Different from the original implementation of the first [GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), here we will not fine-tune the model on a downstream task; we will focus only on the unsupervised pre-training phase. Another important change is that we will use the [Byte-Pair Encoding Tokenizer](https://github.com/omarcastano/solo-learning-nlp/blob/main/self-supervised/Byte_Pair_Encoding_tokenization.ipynb) instead of the SpaCy tokenizer as done in the original paper.\n",
    "\n",
    "\n",
    "**Unsupervised pre-training**: In this phase, **GPT** starts with a corpus of tokens and, moving through it, learns how to predict the next token, given some preceding context. More formally, given an unlabeled corpus of tokens $X = \\{x_1, \\ldots, x_n\\}$, the model learns the conditional probability of predicting token $x_t$ given the preceding $k$ tokens $P(x_t | x_{t-1}, \\ldots, x_{t-k})$ by minimizing the negative log-likelihood\n",
    "\n",
    "$$L(X) = -\\sum_t \\log P(x_t | x_{t-1}, \\ldots, x_{t-k}; \\Theta)$$\n",
    "\n",
    "\n",
    "the conditional probability $P$ is modeled using a neural network with $\\Theta$ represents the model parameters.\n",
    "\n",
    "The neural network model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens:\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://i.imgur.com/dMO2Dtq.png\" alt=\"\" width=\"800px\" height=\"500px\">\n",
    "</div>\n",
    "\n",
    "Where $W_e$ and $W_p$ are the token and positional embedding matrices respectively. $W_f$ is a projection matrix to get the logits. `<s>` denotes the beginning of sentence token and `</s>` denotes the end of sentence token.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# black formatting with jupyter-black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    lab=True,\n",
    "    line_length=140,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We will use the [Wikipedia dataset](https://huggingface.co/datasets/bookcorpus/bookcorpus) to pre-train our Mini GPT model. This dataset is approximately 20GB in size, so please grab a coffee while the data is being downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load_dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not be using the entire dataset due to the extended training time it would require. Instead, we will work with a smaller portion of the original dataset. Feel free to adjust the amount of data used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load de dataset\n",
    "data = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[0:500]\", trust_remote_code=True).to_pandas()\n",
    "# data = load_dataset(\"karpathy/tiny_shakespeare\", split=\"train\", trust_remote_code=True).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a basic data pre-processing function that handles text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "# Basic preprocessing.\n",
    "def text_preprocessing(text: str) -> str:\n",
    "\n",
    "    # to lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove number\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # remove html tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    # remove urls\n",
    "    text = re.sub(r\"https?://\\S+\", \"\", text)\n",
    "\n",
    "    # remove white spaces at the start and the end\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Anarchism</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>anarchism is a political philosophy and moveme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Autism</td>\n",
       "      <td>Autism</td>\n",
       "      <td>autism is a neurodevelopmental disorder charac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Albedo</td>\n",
       "      <td>Albedo</td>\n",
       "      <td>albedo (; ) is the measure of the diffuse refl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>290</td>\n",
       "      <td>https://en.wikipedia.org/wiki/A</td>\n",
       "      <td>A</td>\n",
       "      <td>a, or a, is the first letter and the first vow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>303</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabama</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>alabama () is a state in the southeastern regi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                      url      title  \\\n",
       "0   12  https://en.wikipedia.org/wiki/Anarchism  Anarchism   \n",
       "1   25     https://en.wikipedia.org/wiki/Autism     Autism   \n",
       "2   39     https://en.wikipedia.org/wiki/Albedo     Albedo   \n",
       "3  290          https://en.wikipedia.org/wiki/A          A   \n",
       "4  303    https://en.wikipedia.org/wiki/Alabama    Alabama   \n",
       "\n",
       "                                                text  \n",
       "0  anarchism is a political philosophy and moveme...  \n",
       "1  autism is a neurodevelopmental disorder charac...  \n",
       "2  albedo (; ) is the measure of the diffuse refl...  \n",
       "3  a, or a, is the first letter and the first vow...  \n",
       "4  alabama () is a state in the southeastern regi...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply text normalization\n",
    "data.text = data.text.apply(text_preprocessing)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Custom Tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous notebooks, we have implemented a [Byte-Pair Encoding Tokenizer](https://github.com/omarcastano/solo-learning-nlp/blob/main/self-supervised/Byte_Pair_Encoding_tokenization.ipynb) using only NumPy. However, this Python implementation is quite slow, so we will use the Transformers implementation of the Byte-Pair Encoding Tokenizer to save some time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "import pandas as pd\n",
    "from tokenizers import decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following special tokens:\n",
    "\n",
    "- `<unk>`: Unknown token\n",
    "- `<s>`: Start of sentence token\n",
    "- `</s>`: End of sentence token\n",
    "\n",
    "For the pre-tokenizer, we will use `ByteLevel`, which is very similar to the pre-tokenizer used in the GPT-2 tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "\n",
    "# setting pre-tokenization to ByteLevel this is similar to gpt2 tokenizer\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "\n",
    "# tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Initialize a trainer with desired parameters\n",
    "vocab_size = 30000\n",
    "trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<unk>\", \"<s>\", \"</s>\"])\n",
    "\n",
    "# Load your training data into a list of strings\n",
    "train_data = data.text.tolist()\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(\n",
    "    train_data,\n",
    "    trainer=trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġshe', \"'s\", 'Ġbuying', 'Ġa', 'Ġstair', 'way', 'Ġto', 'Ġheaven', '!', '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick tokenizer test\n",
    "tokenizer.encode(\"she's buying a stairway to heaven!!\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of tokens per instance\n",
    "data[\"text_length\"] = data.text.apply(lambda x: len(tokenizer.encode(x).tokens))\n",
    "data.text_length.describe()\n",
    "data = data.query(\"text_length == 53\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the statistics show above we can se that dataset is composed of wikipedia pages with different lengths, varying from 53 tokens to 25256 with a median of 6834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='text_length', ylabel='Count'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot number of words distribution#mini-gpt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(data=data, x=\"text_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data For Causal Language Modeling\n",
    "\n",
    "In order to prepare our data for causal language modeling, we need to split every Wikipedia page into segments no longer than a defined maximum sequence length (`max_seq_len`). For instance, consider a Wikipedia page with 53 tokens, and assume we set `max_seq_len=12`. This would result in dividing the page into 53//12=4 segments that can be fed into the model for training in batches. It's important to note that 53 divided by 12 equals to 4.416, but using the floor division operator (//) ensures integer division, allowing us to discard the final segment that doesn't reach 14 tokens without the need for padding tokens.\n",
    "\n",
    "The original page containing 53 os the following:\n",
    "\n",
    "**wikipedia_page**: ```trachysomus buquetii is a species of beetle in the family cerambycidae. it was described by james thomson in . it is known from argentina and brazil.\\n\\nreferences\\n\\nonciderini\\nbeetles described in```\n",
    "\n",
    "after splitting the wiki page into 4 segments of 12 tokens we end up with the following table of segments and tokens.\n",
    "\n",
    "|    | text                                        | tokens                                                        |\n",
    "|---:|:--------------------------------------------|:--------------------------------------------------------------|\n",
    "|  0 | \\<s>trachysomus buquetii is a species of\\</s> | [1, 755, 226, 558, 218, 207, 815, 11019, 1103, 231, 168, 1378, 192, 2]  |\n",
    "|  1 | \\<s>beetle in the family cerambycidae. it was\\</s>   | [1, 28906, 194, 172, 1274, 2701, 225, 1198, 33, 6433, 16, 269, 254, 2] |\n",
    "|  2 | \\<s>described by james thomson in . it is known from argentina and\\</s> | [1, 1808, 260, 2429, 14373, 194, 351, 269, 231, 738, 307, 8602, 198, 2]  |\n",
    "|  3 | \\<s> brazil.\\n\\nreferences\\n\\nonciderini\\n\\</s>     | [1, 3959, 16, 132, 132, 1974, 132, 132, 174, 33, 764, 4105, 132, 2]    |\n",
    "\n",
    "\n",
    "Notice that the las part of the sentence is compose by **beetles described in** with only three tokens **[25259, 1808, 194]** so we discard this final segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25259, 1808, 194]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"beetles described in\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trachysomus buquetii is a species of beetle in the family cerambycidae. it was described by james thomson in . it is known from argentina and brazil.\\n\\nreferences\\n\\nonciderini\\nbeetles described in'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.416666666666667"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "53 / 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max sequence length\n",
    "max_seq_len = 14\n",
    "\n",
    "split_tokens_ids = []\n",
    "split_tokens = []\n",
    "\n",
    "# In order to latter add being of sentence token (<s>) and end of sentence token (</s>), we subtract -2 to the max_seq_len\n",
    "seq_len = max_seq_len - 2\n",
    "\n",
    "# Iterate over the entire dataset and generate tokens ids\n",
    "for id in range(len(data)):\n",
    "\n",
    "    tokens = tokenizer.encode(data.text.tolist()[id]).ids\n",
    "\n",
    "    for i in range(len(tokens) // seq_len):\n",
    "\n",
    "        #  Adds the <s> and </s> to the sentences then encode into tokens ids and\n",
    "        token_ids = tokenizer.encode(\"<s>\").ids + tokens[i * seq_len : (i + 1) * seq_len] + tokenizer.encode(\"</s>\").ids\n",
    "        split_tokens_ids.append(token_ids)\n",
    "\n",
    "        # decode tokens_ids\n",
    "        decoded_tokens = tokenizer.decode(split_tokens_ids[i], skip_special_tokens=False)\n",
    "        split_tokens.append(decoded_tokens)\n",
    "\n",
    "\n",
    "# Create a pandas dataframe with the text and tokens ids\n",
    "prepared_data = pd.DataFrame({\"text\": split_tokens, \"tokens\": split_tokens_ids})\n",
    "print(prepared_data.shape)\n",
    "prepared_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Dataset and Data Collator\n",
    "\n",
    "Here we define our custom dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        tokens_ids = self.data.tokens.iloc[idx]\n",
    "        tokens_ids = torch.LongTensor(tokens_ids)\n",
    "\n",
    "        return tokens_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator_for_clm(batch):\n",
    "\n",
    "    tokens_ids = torch.stack(batch)\n",
    "    attention_mask = torch.tril(torch.ones(tokens_ids.shape[0], 1, tokens_ids.shape[1], tokens_ids.shape[1])).bool()\n",
    "\n",
    "    return tokens_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CustomDataset(prepared_data)\n",
    "data_loader = DataLoader(ds, batch_size=2, collate_fn=data_collator_for_clm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids, attention_mask = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini GPT\n",
    "\n",
    "For this implementation we will implement a architecture similar to the one proposed in the [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) original paper\n",
    "\n",
    "<img src=\"https://i.imgur.com/lgoqvjZ.png\" alt= “” width=\"300px\" height=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DecoderTransformer\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, pf_dim, vocab_size, max_seq_length, n_layers, device=\"cpu\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.decoder = DecoderTransformer(embed_dim, num_heads, dropout, pf_dim, vocab_size, max_seq_length, n_layers, device)\n",
    "        self.output = torch.nn.Linear(embed_dim, vocab_size, bias=False).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        # x shape: (batch_size, max_seq_length)\n",
    "        # mask shape: (batch_size, 1, max_seq_length, max_seq_length)\n",
    "\n",
    "        x = self.decoder(x, mask)  # x shape (batch_size, max_seq_length, embedding_dim)\n",
    "        x = self.output(x)  # x shape: (batch_size, max_seq_length, vocab_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def config_training_args(self, lr=2.5e-4, weight_decay=0.1, scheduler=None, scheduler_kwargs={}, gradient_accumulation=1):\n",
    "\n",
    "        self.gradient_accumulation = gradient_accumulation\n",
    "\n",
    "        grouped_params = self.get_grouped_params(weight_decay=weight_decay)\n",
    "        self.optimizer = torch.optim.AdamW(grouped_params, lr=lr)\n",
    "\n",
    "        self.scheduler = scheduler(self.optimizer, **scheduler_kwargs)\n",
    "\n",
    "    def get_grouped_params(self, weight_decay=0.1):\n",
    "\n",
    "        no_decay = [\"bias\", \"layer_norm.weight\", \"embedding\"]\n",
    "        params_with_wd, params_without_wd = [], []\n",
    "\n",
    "        for name, weight in self.named_parameters():\n",
    "\n",
    "            if any([nd in name for nd in no_decay]):\n",
    "                params_without_wd.append(weight)\n",
    "            else:\n",
    "                params_with_wd.append(weight)\n",
    "\n",
    "        return [{\"params\": params_with_wd, \"weight_decay\": weight_decay}, {\"params\": params_without_wd, \"weight_decay\": 0}]\n",
    "\n",
    "    def train_one_epoch(self, train_dataloader):\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        bar = tqdm(train_dataloader, total=len(train_dataloader), leave=True)\n",
    "\n",
    "        # set zero grad\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        for step, (token_ids, attention_mask) in enumerate(bar, 1):\n",
    "\n",
    "            # move to the correct device\n",
    "            token_ids, attention_mask = token_ids.to(self.device), attention_mask.to(self.device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = self(token_ids, attention_mask)\n",
    "\n",
    "            # remove begin of sentence token from labels\n",
    "            labels = token_ids[:, 1:]\n",
    "\n",
    "            # remove end_of_sentence token from outputs\n",
    "            outputs = outputs[:, :-1, :]\n",
    "\n",
    "            # reshape outputs and labels\n",
    "            labels = labels.reshape(-1)\n",
    "            outputs = outputs.reshape(-1, outputs.shape[2])\n",
    "\n",
    "            # compute loss\n",
    "            loss = F.cross_entropy(outputs, labels) / self.gradient_accumulation\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # apply gradient accumulation\n",
    "            if step % self.gradient_accumulation == 0:\n",
    "\n",
    "                # apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
    "\n",
    "                # update weights\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # apply scheduler\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                # set zero grad\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            # running loss\n",
    "            running_loss += loss.item() * self.gradient_accumulation\n",
    "\n",
    "            # print statistics\n",
    "            bar.set_description(f\"Train loss: {running_loss/step:.5f}\")\n",
    "\n",
    "    def train(self, train_dataloader, epochs):\n",
    "\n",
    "        bar = tqdm(range(1, epochs + 1), total=epochs, leave=True)\n",
    "\n",
    "        for epoch in bar:\n",
    "\n",
    "            self.train_one_epoch(train_dataloader)\n",
    "            bar.set_description(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, sentence, tokenizer, temperature):\n",
    "\n",
    "        sentence = \"<s> \" + sentence\n",
    "\n",
    "        tokens_ids = torch.LongTensor(tokenizer.encode(sentence).ids).unsqueeze(0).to(\"cuda\")\n",
    "        attention_mask = torch.ones(tokens_ids.shape[0], 1, tokens_ids.shape[1], tokens_ids.shape[1]).bool().to(\"cuda\")\n",
    "\n",
    "        for _ in range(tokens_ids.shape[-1], self.max_seq_length):\n",
    "\n",
    "            # compute logits\n",
    "            logits = self(tokens_ids, attention_mask)\n",
    "\n",
    "            # scale logits with temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            # computes probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # sample new token using a multinomial distribution\n",
    "            new_token = torch.multinomial(probs, 1)\n",
    "\n",
    "            # update tokens ids with the new predicted token\n",
    "            tokens_ids = torch.cat((tokens_ids, new_token), dim=-1)\n",
    "\n",
    "            # updates the attention mask\n",
    "            attention_mask = torch.ones(tokens_ids.shape[0], 1, tokens_ids.shape[1], tokens_ids.shape[1]).bool().to(\"cuda\")\n",
    "\n",
    "            if new_token == tokenizer.encode(\"</s>\").ids[0]:\n",
    "                break\n",
    "\n",
    "        prediction = tokens_ids.cpu().squeeze(0).tolist()\n",
    "        prediction = re.sub(r\"\\s(?!Ñ)\", \"\", tokenizer.decode(prediction, skip_special_tokens=False)).replace(\"Ñ\", \"\")\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the DecoderTransformer layer\n",
    "embed_dim = 768\n",
    "num_heads = 12\n",
    "dropout = 0.1\n",
    "pf_dim = 3072\n",
    "bs = 32\n",
    "n_layers = 12\n",
    "n_epochs = 5\n",
    "\n",
    "gpt = MiniGPT(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    dropout=dropout,\n",
    "    pf_dim=pf_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_length=max_seq_len,\n",
    "    n_layers=n_layers,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CustomDataset(prepared_data)\n",
    "data_loader = DataLoader(ds, batch_size=bs, collate_fn=data_collator_for_clm, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confing model training args\n",
    "gradient_accumulation = 8\n",
    "lr = 2.5e-4\n",
    "weight_decay = 0.1\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "scheduler_kwargs = {\"T_max\": len(data_loader) * n_epochs // gradient_accumulation, \"eta_min\": 1e-6}\n",
    "\n",
    "\n",
    "gpt.config_training_args(lr, weight_decay, scheduler, scheduler_kwargs, gradient_accumulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.train(data_loader, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.generate(\"anarchism is a political philosophy and\", tokenizer, temperature=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
