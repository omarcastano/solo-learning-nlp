{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Generative Pretrained Transformer-1 (mGPT-1)\n",
    "\n",
    "This notebook illustrates a basic implementation of the Generative Pretrained Transformer model using PyTorch. We follow a similar approach as described in the original [GPT-1 paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf). Further details about the model architecture will be provided in the Mini GPT Architecture section. The specific sections covered in this notebook are:\n",
    "\n",
    "1. Data Downloading and Normalization\n",
    "2. Training Custom Tokenizer\n",
    "3. wikipedia Pages Splitting\n",
    "4. Building Custom Dataset and Data Loader\n",
    "5. Mini GPT Architecture\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "One of the most well-known successes of transformer-based language models has been text generation with the GPT-2, GPT-3, and GPT-4 language models, which are built from stacks of transformer-decoder layers. GPT (Generative Pre-training) is a family of generative pre-training models that are part of the language modeling trend where a model is first trained on unsupervised data, in a task-agnostic fashion, and later fine-tuned for a specific task. The first model in this family, the eponymous [GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), pre-trains a stack of transformer decoder layers on a large body of unlabeled text and is then fine-tuned on labeled, task-specific data. GPT is an autoregressive model, which means it uses inputs from previous steps of a sequence to predict values later in the sequence.\n",
    "\n",
    "Different from the original implementation of the first [GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), here we will not fine-tune the model on a downstream task; we will focus only on the unsupervised pre-training phase. Another important change is that we will use the [Byte-Pair Encoding Tokenizer](https://github.com/omarcastano/solo-learning-nlp/blob/main/self-supervised/Byte_Pair_Encoding_tokenization.ipynb) instead of the SpaCy tokenizer as done in the original paper.\n",
    "\n",
    "\n",
    "**Unsupervised pre-training**: In this phase, **GPT** starts with a corpus of tokens and, moving through it, learns how to predict the next token, given some preceding context. More formally, given an unlabeled corpus of tokens $X = \\{x_1, \\ldots, x_n\\}$, the model learns the conditional probability of predicting token $x_t$ given the preceding $k$ tokens $P(x_t | x_{t-1}, \\ldots, x_{t-k})$ by minimizing the negative log-likelihood\n",
    "\n",
    "$$L(X) = -\\sum_t \\log P(x_t | x_{t-1}, \\ldots, x_{t-k}; \\Theta)$$\n",
    "\n",
    "\n",
    "the conditional probability $P$ is modeled using a neural network with $\\Theta$ represents the model parameters.\n",
    "\n",
    "The neural network model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens:\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://i.imgur.com/dMO2Dtq.png\" alt=\"\" width=\"800px\" height=\"500px\">\n",
    "</div>\n",
    "\n",
    "Where $W_e$ and $W_p$ are the token and positional embedding matrices respectively. $W_f$ is a projection matrix to get the logits. `<s>` denotes the beginning of sentence token and `</s>` denotes the end of sentence token.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# black formatting with jupyter-black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    lab=True,\n",
    "    line_length=170,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We will use the [Wikipedia dataset](https://huggingface.co/datasets/bookcorpus/bookcorpus) to pre-train our Mini GPT model. This dataset is approximately 20GB in size, so please grab a coffee while the data is being downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load_dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not be using the entire dataset due to the extended training time it would require. Instead, we will work with a smaller portion of the original dataset. Feel free to adjust the amount of data used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load de dataset\n",
    "data = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[0:100]\", trust_remote_code=True).to_pandas()\n",
    "# data = load_dataset(\"karpathy/tiny_shakespeare\", split=\"train\", trust_remote_code=True).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a basic data pre-processing function that handles text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "# Basic preprocessing.\n",
    "def text_preprocessing(text: str) -> str:\n",
    "\n",
    "    # to lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove number\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # remove html tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    # remove urls\n",
    "    text = re.sub(r\"https?://\\S+\", \"\", text)\n",
    "\n",
    "    # remove white spaces at the start and the end\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply text normalization\n",
    "data.text = data.text.apply(text_preprocessing)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Custom Tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous notebooks, we have implemented a [Byte-Pair Encoding Tokenizer](https://github.com/omarcastano/solo-learning-nlp/blob/main/self-supervised/Byte_Pair_Encoding_tokenization.ipynb) using only NumPy. However, this Python implementation is quite slow, so we will use the Transformers implementation of the Byte-Pair Encoding Tokenizer to save some time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "import pandas as pd\n",
    "from tokenizers import decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following special tokens:\n",
    "\n",
    "- `<unk>`: Unknown token\n",
    "- `<s>`: Start of sentence token\n",
    "- `</s>`: End of sentence token\n",
    "\n",
    "For the pre-tokenizer, we will use `ByteLevel`, which is very similar to the pre-tokenizer used in the GPT-2 tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "\n",
    "# setting pre-tokenization to ByteLevel this is similar to gpt2 tokenizer\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "\n",
    "# tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Initialize a trainer with desired parameters\n",
    "vocab_size = 30000\n",
    "trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<unk>\", \"<s>\", \"</s>\"])\n",
    "\n",
    "# Load your training data into a list of strings\n",
    "train_data = data.text.tolist()\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(\n",
    "    train_data,\n",
    "    trainer=trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick tokenizer test\n",
    "tokenizer.encode(\"she's buying a stairway to heaven!!\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of tokens per instance\n",
    "data[\"text_length\"] = data.text.apply(lambda x: len(tokenizer.encode(x).tokens))\n",
    "data.text_length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the statistics show above we can se that dataset is composed of wikipedia pages with different lengths, varying from 53 tokens to 25256 with a median of 6834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of words distribution#mini-gpt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(data=data, x=\"text_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wikipedia Pages Splitting\n",
    "\n",
    "In order to prepare our data for causal language modeling, we need to split every Wikipedia page into segments no longer than a defined maximum sequence length (`max_seq_len`). For instance, consider a Wikipedia page with 53 tokens, and assume we set `max_seq_len=12`. This would result in dividing the page into 53//12=4 segments that can be fed into the model for training in batches. It's important to note that 53 divided by 12 equals to 4.416, but using the floor division operator (//) ensures integer division, allowing us to discard the final segment that doesn't reach 14 tokens without the need for padding tokens.\n",
    "\n",
    "The original page containing 53 os the following:\n",
    "\n",
    "**wikipedia_page**: ```trachysomus buquetii is a species of beetle in the family cerambycidae. it was described by james thomson in . it is known from argentina and brazil.\\n\\nreferences\\n\\nonciderini\\nbeetles described in```\n",
    "\n",
    "after splitting the wiki page into 4 segments of 12 tokens we end up with the following table of segments and tokens.\n",
    "\n",
    "|    | text                                        | tokens                                                        |\n",
    "|---:|:--------------------------------------------|:--------------------------------------------------------------|\n",
    "|  0 | \\<s>trachysomus buquetii is a species of\\</s> | [1, 755, 226, 558, 218, 207, 815, 11019, 1103, 231, 168, 1378, 192, 2]  |\n",
    "|  1 | \\<s>beetle in the family cerambycidae. it was\\</s>   | [1, 28906, 194, 172, 1274, 2701, 225, 1198, 33, 6433, 16, 269, 254, 2] |\n",
    "|  2 | \\<s>described by james thomson in . it is known from argentina and\\</s> | [1, 1808, 260, 2429, 14373, 194, 351, 269, 231, 738, 307, 8602, 198, 2]  |\n",
    "|  3 | \\<s> brazil.\\n\\nreferences\\n\\nonciderini\\n\\</s>     | [1, 3959, 16, 132, 132, 1974, 132, 132, 174, 33, 764, 4105, 132, 2]    |\n",
    "\n",
    "\n",
    "Notice that the remaining segment is given by **beetles described in** with only three tokens **[25259, 1808, 194]** so we discard this final segment.\n",
    "\n",
    "\n",
    "In our implementation we will define `max_seq_len=128`, with the begin of sentence token `<s>` and end of sentence token `</s>` counting as part of the 128 token in each segment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max sequence length\n",
    "max_seq_len = 128\n",
    "\n",
    "split_tokens_ids = []\n",
    "split_tokens = []\n",
    "\n",
    "# In order to latter add being of sentence token (<s>) and end of sentence token (</s>), we subtract -2 to the max_seq_len\n",
    "seq_len = max_seq_len - 2\n",
    "\n",
    "# Iterate over the entire dataset and generate tokens ids\n",
    "for id in range(len(data)):\n",
    "\n",
    "    tokens = tokenizer.encode(data.text.tolist()[id]).ids\n",
    "\n",
    "    for i in range(len(tokens) // seq_len):\n",
    "\n",
    "        #  Adds the <s> and </s> to the sentences then encode into tokens ids and\n",
    "        token_ids = tokenizer.encode(\"<s>\").ids + tokens[i * seq_len : (i + 1) * seq_len] + tokenizer.encode(\"</s>\").ids\n",
    "        split_tokens_ids.append(token_ids)\n",
    "\n",
    "        # decode tokens_ids\n",
    "        decoded_tokens = tokenizer.decode(split_tokens_ids[i], skip_special_tokens=False)\n",
    "        split_tokens.append(decoded_tokens)\n",
    "\n",
    "\n",
    "# Create a pandas dataframe with the text and tokens ids\n",
    "prepared_data = pd.DataFrame({\"text\": split_tokens, \"tokens\": split_tokens_ids})\n",
    "print(prepared_data.shape)\n",
    "prepared_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Dataset and Data Collator\n",
    "\n",
    "Here we define our custom Dataset and Data Collator which in this case are very simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Dataset and DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        tokens_ids = self.data.tokens.iloc[idx]\n",
    "        tokens_ids = torch.LongTensor(tokens_ids)\n",
    "\n",
    "        return tokens_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Collator**: In our implementation the data collator is very simple but at the same time very important because we will use it to generate the attention masks. The following is a description of how we generate the attention mask and why they are important in causal language modeling. For more details about the Transformer and self-attention mechanism, visit the [NMT with transformers notebook]().\n",
    "\n",
    "\n",
    "In causal language modeling, such as in GPT, the attention mask is crucial for ensuring that during training the model attends only to previous tokens during generation and not to tokens that come after in the sequence. For instance, consider the sentence: **\"I love reading notebooks to learn about transformers\"**. \n",
    "\n",
    "To predict the token **\"learn\"**, we want the model to only have access to the preceding tokens **\"I love reading notebooks to\"**. This behavior is controlled using the attention mask.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://i.imgur.com/bFbKM3Q.png\" alt=\"\" width=\"900px\" height=\"250px\">\n",
    "</div>\n",
    "\n",
    "The process described is handled by the **Masked Multi-Head Self Attention**, which is given by\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{MaskedAttention}(Q, K, V) = \\text{SoftMax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation*}\n",
    "\n",
    "where $Q$ denotes the queries, $K$ the keys, and $V$ the values. $d_k$ denotes the embedding dimension and $M$ is the **Attention Mask**.\n",
    "\n",
    "As you can see, the **Attention Mask** is inserted after the scaling of the multiplication of $Q$ and $K^T$ and before the $softmax$, so that the softmax results in the actual scaled values for previous tokens and the value 0 for future tokens. For more details about the Transformer and self-attention mechanism, visit the [NMT with transformers notebook]().\n",
    "\n",
    "**Attention Mask For Causal Language Modeling**: The attention mask is usually a lower triangular matrix where the entries below the main diagonal are set to a large negative value (e.g., `-inf` or `-1e9`), and the rest are set to zero or a small value (like `-1e5`). This ensures that the model attends to tokens that precede the current token being generated and ignores tokens that follow it. Using our current example sentence, the attention mask should look like the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "M = \\begin{bmatrix}\n",
    "  0 & -\\text{inf} & -\\text{inf} & -\\text{inf} & -\\text{inf} & -\\text{inf} & -\\text{inf} & -\\text{inf} \\\\\n",
    "  0 & 0 & -\\text{inf} & -\\text{inf} & -\\text{inf} & -\\text{inf} & -\\text{inf} & -\\text{inf} \\\\\n",
    "  0 & 0 & 0 & -\\text{inf} & -\\text{inf} & -\\text{inf} & -\\text{inf} & -\\text{inf} \\\\\n",
    "  0 & 0 & 0 & 0 & -\\text{inf} & -\\text{inf} & -\\text{inf} & -\\text{inf} \\\\\n",
    "  0 & 0 & 0 & 0 & 0 & -\\text{inf} & -\\text{inf} & -\\text{inf} \\\\\n",
    "  0 & 0 & 0 & 0 & 0 & 0 & -\\text{inf} & -\\text{inf} \\\\\n",
    "  0 & 0 & 0 & 0 & 0 & 0 & 0 & -\\text{inf} \\\\\n",
    "  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "In our PyTorch implementation, the attention masks generated in the `data_collator` will be a little different. The attention mask in the `data_collator` will be $1$ for past tokens and $0$ for future tokens, as shown below:\n",
    "\n",
    "\\begin{equation*}\n",
    "M = \\begin{bmatrix}\n",
    "  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "  1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "  1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "  1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "  1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 \\\\\n",
    "  1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 \\\\\n",
    "  1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 \\\\\n",
    "  1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Then, within the `MultiHeadAttention`, we will use `torch.Tensor.masked_fill(M == 0, -float(\"inf\"))` to get the attention mask.\n",
    "**Attention Mask**: In the data collator, we will also generate the attention masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom data collator\n",
    "def data_collator_for_clm(batch):\n",
    "\n",
    "    # get token ids\n",
    "    tokens_ids = torch.stack(batch)\n",
    "\n",
    "    # generate the attention mask for causal language modeling\n",
    "    attention_mask = torch.tril(torch.ones(tokens_ids.shape[0], 1, tokens_ids.shape[1], tokens_ids.shape[1])).bool()\n",
    "\n",
    "    return tokens_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the custom dataset and data collator\n",
    "ds = CustomDataset(prepared_data)\n",
    "data_loader = DataLoader(ds, batch_size=2, collate_fn=data_collator_for_clm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some tokens_ids and attention mask\n",
    "token_ids, attention_mask = next(iter(data_loader))\n",
    "print(token_ids.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a attention mask\n",
    "attention_mask[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini GPT-1\n",
    "\n",
    "The mini GPT-1 implementation is composed of N-layer decoder-only transformer with masked self-attention heads similar to the one proposed in the [GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) original paper. The decoder block used in this notebook is very similar to the original [transformer](https://arxiv.org/abs/1706.03762) and is illustrated in the following image:\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://i.imgur.com/7PCqCB7.png\" alt=\"\" width=\"500px\" height=\"550px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text & Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input embedding are compose of text/token embeddings and positional embeddings. We used learned position embeddings instead of the sinusoidal version proposed in the original work.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\text{embedding}(x) &= \\operatorname{text\\_embeddings}(x) + \\operatorname{positional\\_embeddings}(x) \\\\\n",
    "                    &= W_e \\cdot x^T + W_p \\cdot x_{pos}^T  \n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "where \n",
    "- $W_e$ is a matrix of shape $\\text{max\\_seq\\_len} \\times \\text{embed\\_dim}$\n",
    "- $x$ is a matrix of shape $\\text{batch\\_size} \\times \\text{seq\\_len}$\n",
    "- $W_p$ is a matrix of shape $\\text{max\\_seq\\_len} \\times \\text{embed\\_dim}$ containing the positional encodings\n",
    "- $x_{pos}$ is a matrix of shape $\\text{batch\\_size} \\times \\text{seq\\_len}$ containing position indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding class using positional and token embeddings\n",
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding class for BERT-like models.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        vocab_size: int\n",
    "            Size of the vocabulary.\n",
    "        embed_dim: int\n",
    "            Size of the embedding dimension.\n",
    "        max_seq_len: int\n",
    "            Maximum sequence length.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_len, device=\"cpu\"):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_dim, device=device)\n",
    "        self.positional_encodings = nn.Embedding(max_seq_len, embed_dim, device=device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        token_embeddings = self.token_embeddings(x)\n",
    "\n",
    "        x_pos = torch.arange(x.shape[1]).unsqueeze(0).expand(x.shape[0], -1).to(self.device)\n",
    "\n",
    "        positional_encodings = self.positional_encodings(x_pos)\n",
    "\n",
    "        embedding = token_embeddings + positional_encodings\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Embedding class\n",
    "vocab_size = 10\n",
    "embed_dim = 8\n",
    "max_seq_len = 3\n",
    "bs = 2\n",
    "\n",
    "x = torch.randint(0, vocab_size - 1, size=(bs, max_seq_len))\n",
    "\n",
    "embedding = Embedding(vocab_size, embed_dim, max_seq_len)\n",
    "embedding(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same multi-head self attention mechanism proposed in the original transformer paper\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://i.imgur.com/I8ouVdr.png\" alt=\"\" width=\"700px\" height=\"450px\">\n",
    "</div>\n",
    "\n",
    "Mathematically, we have the query, key, and value;  $ Q \\in \\mathbb{R}^{B \\times L \\times D} $, $ K \\in \\mathbb{R}^{B \\times L \\times D} $, $ V \\in \\mathbb{R}^{B \\times L \\times D} $, where $ B $ is the batch size, $ L $ is the sequence length, and $ D $ is the embedding dimension. \n",
    "\n",
    "The output of the $i-th$ head, that is the scaled dot product attention, is given by\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "Z_i &= \\operatorname{Attention}\\left(Q \\cdot W_q^i, K \\cdot W_k^i, V \\cdot W_v^i\\right) \\\\\n",
    "&= \\operatorname{softmax}\\left(\\frac{Q' \\cdot K'^{\\mathrm{T}} + \\mathbf{M}}{\\sqrt{d_k}} \\right) V\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "where \n",
    "- $ W_q^i, W_k^i, W_v^i \\in \\mathbb{R}^{D \\times K} $ are learnable weights for the $i-th$ head with $K=\\frac{D}{\\text{h}}$ being the dimension of each head, here h denotes the number of attention heads.\n",
    "- $Q' = Q \\cdot W_q, \\quad K' = K \\cdot W_k, \\quad V' = V \\cdot W_v$ are linear projection.\n",
    "\n",
    "\n",
    "Multi-head attention have multiple sets of query/key/value weight matrices, each resulting in different query/key/value linear projection matrices for the inputs, finally generating output matrices $Z_i$ . These output matrices from each head are concatenated and multiplied with an additional weight matrix, $W_O \\in \\mathbb{R}^{D \\times D}$  , to get a single final matrix, $Z$, with vectors zi as output for each input $x_i$ .The MultiHead\n",
    "\n",
    "$$\\operatorname{multihead}(Q, K, V)=W_O \\text { concat }\\left(Z_1, \\ldots, Z_h\\right)$$\n",
    "\n",
    "For more details about the Transformer and self-attention mechanism, visit the [NMT with transformers notebook]().\n",
    "\n",
    "Now that we are clear with the maths, let's get hands-on with the implementation in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define MultiHeadAttention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_head = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.w_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.w_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.w_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.w_o = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q shape: (batch_size, seq_len, embed_dim)\n",
    "        # k shape: (batch_size, seq_len, embed_dim)\n",
    "        # v shape: (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        batch_size = q.shape[0]\n",
    "        seq_len = q.shape[1]\n",
    "\n",
    "        Q = self.w_q(q)  # Q shape : (batch_size, seq_len, embed_dim)\n",
    "        K = self.w_k(k)  # K shape : (batch_size, seq_len, embed_dim)\n",
    "        V = self.w_v(v)  # V shape : (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        Q = Q.reshape(batch_size, seq_len, self.num_head, self.head_dim).permute(0, 2, 1, 3)  # Q shape: (batch_size, num_head, seq_len, head_dim)\n",
    "        K = K.reshape(batch_size, seq_len, self.num_head, self.head_dim).permute(0, 2, 1, 3)  # K shape: (batch_size, num_head, seq_len, head_dim)\n",
    "        V = V.reshape(batch_size, seq_len, self.num_head, self.head_dim).permute(0, 2, 1, 3)  # V shape: (batch_size, num_head, seq_len, head_dim)\n",
    "\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / np.sqrt(self.head_dim)  # energy shape: (batch_size, num_head, seq_len, seq_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -float(\"inf\"))\n",
    "\n",
    "        attention = torch.matmul(F.softmax(energy, dim=-1), V)  # attention shape: (batch_size, num_head, seq_len, head_dim)\n",
    "\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        Z = self.w_o(attention.permute(0, 2, 1, 3).reshape(batch_size, seq_len, self.num_head * self.head_dim))  # Z shape: (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        return Z, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the MultiHeadAttention\n",
    "bs = 2\n",
    "embed_dim = 4\n",
    "max_length = 5\n",
    "num_heads = 1\n",
    "\n",
    "X = torch.rand(size=(bs, max_length, embed_dim))\n",
    "mask = torch.tril(torch.ones(max_length, max_length))\n",
    "multi_head_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "z, attention = multi_head_attention(X, X, X, mask)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals and Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to ResNets, the inputs, $X$, are short circuited to the output, $Z$, and both are added and passed through layer normalization \n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\n",
    "\\begin{aligned}\n",
    "Z' &= \\operatorname{AddAndNorm}(X, Z)\\\\\n",
    "&= \\operatorname{LayerNorm(X + Z)}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Where $X$ are the token embeddings and $Z$ is the output of the Multi-Head Self Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define add and normalize layer\n",
    "class AddAndNormalize(nn.Module):\n",
    "    def __init__(self, embed_dim) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        return self.layer_norm(x + z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positionwise Feed-forward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder contain a fully connected feed-forward network after the first Layer Normalization. Following the GPT-1 implementation we will use **GELU** activation function instead of **ReLU**.\n",
    "\n",
    "$$FFN(x) = GELU(x \\cdot W_1 + b_1 ) \\cdot W_2 + b_2$$\n",
    "\n",
    "where $W_1 \\in \\mathbb{R}^{D \\times F}$, $b_1 \\in \\mathbb{R}^{F}$, $W_2 \\in \\mathbb{R}^{F \\times D}$ and $b_ \\in \\mathbb{R}^{D}$ are learnable weights with $D$ denting the embedding dimension and $F$ denoting the feed-forward dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define PositionWise FFN class\n",
    "class PositionWiseFFN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, pf_dim, dropout) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.w1 = nn.Linear(embed_dim, pf_dim)\n",
    "        self.w2 = nn.Linear(pf_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        x = self.w1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.w2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block\n",
    "\n",
    "Now we have all the the elements to build our Decoder Block.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://i.imgur.com/u56urY1.png\" alt=\"\" width=\"500px\" height=\"550px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define modified decoder layer\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_attention = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.feedforward = PositionWiseFFN(embed_dim, pf_dim, dropout)\n",
    "\n",
    "        self.add_and_norm_1 = AddAndNormalize(embed_dim)\n",
    "        self.add_and_norm_2 = AddAndNormalize(embed_dim)\n",
    "\n",
    "    def forward(self, x, decoder_mask):\n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        # decoder_mask shape: (batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "        z, _ = self.mask_attention(x, x, x, mask=decoder_mask)\n",
    "        x = self.add_and_norm_1(x, z)\n",
    "\n",
    "        z = self.feedforward(x)\n",
    "        x = self.add_and_norm_2(x, z)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the DecoderLayer\n",
    "embed_dim = 8\n",
    "max_length = 5\n",
    "num_heads = 2\n",
    "dropout = 0.5\n",
    "pf_dim = 4\n",
    "vocab_size = 10\n",
    "bs = 2\n",
    "\n",
    "\n",
    "x = torch.randint(0, vocab_size, size=(bs, max_length))\n",
    "x_embeddings = torch.rand(size=(bs, max_length, embed_dim))\n",
    "decoder_mask = torch.tril(torch.ones(size=(bs, 1, max_length, max_length)))\n",
    "\n",
    "\n",
    "decoder_block = DecoderBlock(embed_dim, num_heads, pf_dim, dropout)\n",
    "z, attention = decoder_block(x_embeddings, decoder_mask)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini GPT-1 Model\n",
    "\n",
    "Now we will build our Mini GPT model, the model specification are the following\n",
    "\n",
    "**mini GPT-1 Model specifications1**\n",
    "\n",
    "1. We will use gradient accumulation to supply the lack of VRAM. \n",
    "2. We use L2 regularization over all weights except \"bias\", \"layer_norm\" and \"embedding\".\n",
    "3. We will use CosineAnnealingLR as our learning rate scheduler. \n",
    "\n",
    "\n",
    "Out mini GPT model is defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\operatorname{embeddings} &= \\operatorname{Embedding}(x) \\\\\n",
    "Z &= \\operatorname{GPTDecoder(embeddings)} \\# \\text{N times} \\\\\n",
    "logtis &= Z \\cdot W_o\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $W_o$ are learnable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, pf_dim, vocab_size, max_seq_length, n_layers, device=\"cpu\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, embed_dim, max_seq_length, device)\n",
    "        self.decoder = nn.ModuleList([DecoderBlock(embed_dim, num_heads, pf_dim, dropout) for _ in range(n_layers)]).to(device)\n",
    "        self.output = torch.nn.Linear(embed_dim, vocab_size, bias=False).to(device)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        # x shape: (batch_size, max_seq_length)\n",
    "        # mask shape: (batch_size, 1, max_seq_length, max_seq_length)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x, mask)  # x shape (batch_size, max_seq_length, embedding_dim)\n",
    "\n",
    "        x = self.output(x)  # x shape: (batch_size, max_seq_length, vocab_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def config_training_args(self, lr=2.5e-4, weight_decay=0.1, scheduler=None, scheduler_kwargs={}, gradient_accumulation=1):\n",
    "\n",
    "        self.gradient_accumulation = gradient_accumulation\n",
    "\n",
    "        grouped_params = self.get_grouped_params(weight_decay=weight_decay)\n",
    "        self.optimizer = torch.optim.AdamW(grouped_params, lr=lr)\n",
    "\n",
    "        self.scheduler = scheduler(self.optimizer, **scheduler_kwargs)\n",
    "\n",
    "    def get_grouped_params(self, weight_decay=0.1):\n",
    "\n",
    "        no_decay = [\"bias\", \"layer_norm.weight\", \"embedding\"]\n",
    "        params_with_wd, params_without_wd = [], []\n",
    "\n",
    "        for name, weight in self.named_parameters():\n",
    "\n",
    "            if any([nd in name for nd in no_decay]):\n",
    "                params_without_wd.append(weight)\n",
    "            else:\n",
    "                params_with_wd.append(weight)\n",
    "\n",
    "        return [{\"params\": params_with_wd, \"weight_decay\": weight_decay}, {\"params\": params_without_wd, \"weight_decay\": 0}]\n",
    "\n",
    "    def train_one_epoch(self, train_dataloader):\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        bar = tqdm(train_dataloader, total=len(train_dataloader), leave=True)\n",
    "\n",
    "        # set zero grad\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        for step, (token_ids, attention_mask) in enumerate(bar, 1):\n",
    "\n",
    "            # move to the correct device\n",
    "            token_ids, attention_mask = token_ids.to(self.device), attention_mask.to(self.device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = self(token_ids, attention_mask)\n",
    "\n",
    "            # remove begin of sentence token from labels\n",
    "            labels = token_ids[:, 1:]\n",
    "\n",
    "            # remove end_of_sentence token from outputs\n",
    "            outputs = outputs[:, :-1, :]\n",
    "\n",
    "            # reshape outputs and labels\n",
    "            labels = labels.reshape(-1)\n",
    "            outputs = outputs.reshape(-1, outputs.shape[2])\n",
    "\n",
    "            # compute loss\n",
    "            loss = F.cross_entropy(outputs, labels) / self.gradient_accumulation\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # apply gradient accumulation\n",
    "            if step % self.gradient_accumulation == 0:\n",
    "\n",
    "                # apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
    "\n",
    "                # update weights\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # apply scheduler\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                # set zero grad\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            # running loss\n",
    "            running_loss += loss.item() * self.gradient_accumulation\n",
    "\n",
    "            # print statistics\n",
    "            bar.set_description(f\"Train loss: {running_loss/step:.5f}\")\n",
    "\n",
    "    def train(self, train_dataloader, epochs):\n",
    "\n",
    "        bar = tqdm(range(1, epochs + 1), total=epochs, leave=True)\n",
    "\n",
    "        for epoch in bar:\n",
    "\n",
    "            self.train_one_epoch(train_dataloader)\n",
    "            bar.set_description(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, sentence, tokenizer, temperature):\n",
    "\n",
    "        sentence = \"<s> \" + sentence\n",
    "\n",
    "        tokens_ids = torch.LongTensor(tokenizer.encode(sentence).ids).unsqueeze(0).to(\"cuda\")\n",
    "        attention_mask = torch.ones(tokens_ids.shape[0], 1, tokens_ids.shape[1], tokens_ids.shape[1]).bool().to(\"cuda\")\n",
    "\n",
    "        for _ in range(tokens_ids.shape[-1], self.max_seq_length):\n",
    "\n",
    "            # compute logits\n",
    "            logits = self(tokens_ids, attention_mask)\n",
    "\n",
    "            # scale logits with temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            # computes probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # sample new token using a multinomial distribution\n",
    "            new_token = torch.multinomial(probs, 1)\n",
    "\n",
    "            # update tokens ids with the new predicted token\n",
    "            tokens_ids = torch.cat((tokens_ids, new_token), dim=-1)\n",
    "\n",
    "            # updates the attention mask\n",
    "            attention_mask = torch.ones(tokens_ids.shape[0], 1, tokens_ids.shape[1], tokens_ids.shape[1]).bool().to(\"cuda\")\n",
    "\n",
    "            if new_token == tokenizer.encode(\"</s>\").ids[0]:\n",
    "                break\n",
    "\n",
    "        prediction = tokens_ids.cpu().squeeze(0).tolist()\n",
    "        prediction = tokenizer.decode(prediction, skip_special_tokens=False)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the DecoderTransformer layer\n",
    "embed_dim = 768\n",
    "num_heads = 12\n",
    "dropout = 0.1\n",
    "pf_dim = 3072\n",
    "bs = 32\n",
    "n_layers = 12\n",
    "n_epochs = 1\n",
    "max_seq_len = 128\n",
    "vocab_size = 30522\n",
    "\n",
    "gpt = MiniGPT(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    dropout=dropout,\n",
    "    pf_dim=pf_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_length=max_seq_len,\n",
    "    n_layers=n_layers,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CustomDataset(prepared_data)\n",
    "data_loader = DataLoader(ds, batch_size=bs, collate_fn=data_collator_for_clm, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confing model training args\n",
    "gradient_accumulation = 8\n",
    "lr = 2.5e-4\n",
    "weight_decay = 0.1\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "scheduler_kwargs = {\"T_max\": len(data_loader) * n_epochs // gradient_accumulation, \"eta_min\": 1e-6}\n",
    "\n",
    "\n",
    "gpt.config_training_args(lr, weight_decay, scheduler, scheduler_kwargs, gradient_accumulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.train(data_loader, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.generate(\"anarchism is a political philosophy and\", tokenizer, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
