{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Generative Pretrained Transformer (GPT)\n",
    "\n",
    "This notebook illustrates a basic implementation of the Generative Pretrained Transformer using PyTorch. We follow a similar approach as described in the original [GPT paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf). Further details about the model architecture will be provided in the [Mini GPT Architecture](#mini-gpt) section. The specific section cover in this notebook are:\n",
    "\n",
    "1. [Data preparation](#data-preparation) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# black formatting with jupyter-black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    lab=True,\n",
    "    line_length=140,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "## Data preparation\n",
    "\n",
    "\n",
    "Wi will use [wikipedia](https://huggingface.co/datasets/bookcorpus/bookcorpus) as data to pre-train our mini-gpt model, this dataset is about 20GB storage so grab a coffee while data is downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load_dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not work with the entire dataset as the training may take weeks, so we will work with a portion of the original dataset, feel free to increase or decrease the amount of data used during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load de dataset\n",
    "data = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[0:500]\", trust_remote_code=True).to_pandas()\n",
    "# data = load_dataset(\"karpathy/tiny_shakespeare\", split=\"train\", trust_remote_code=True).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizer dataset\n",
    "from utils import text_preprocessing\n",
    "\n",
    "data.text = data.text.apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Anarchism</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>anarchism is a political philosophy and moveme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Autism</td>\n",
       "      <td>Autism</td>\n",
       "      <td>autism is a neurodevelopmental disorder charac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Albedo</td>\n",
       "      <td>Albedo</td>\n",
       "      <td>albedo (; ) is the measure of the diffuse refl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>290</td>\n",
       "      <td>https://en.wikipedia.org/wiki/A</td>\n",
       "      <td>A</td>\n",
       "      <td>a, or a, is the first letter and the first vow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>303</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabama</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>alabama () is a state in the southeastern regi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                      url      title  \\\n",
       "0   12  https://en.wikipedia.org/wiki/Anarchism  Anarchism   \n",
       "1   25     https://en.wikipedia.org/wiki/Autism     Autism   \n",
       "2   39     https://en.wikipedia.org/wiki/Albedo     Albedo   \n",
       "3  290          https://en.wikipedia.org/wiki/A          A   \n",
       "4  303    https://en.wikipedia.org/wiki/Alabama    Alabama   \n",
       "\n",
       "                                                text  \n",
       "0  anarchism is a political philosophy and moveme...  \n",
       "1  autism is a neurodevelopmental disorder charac...  \n",
       "2  albedo (; ) is the measure of the diffuse refl...  \n",
       "3  a, or a, is the first letter and the first vow...  \n",
       "4  alabama () is a state in the southeastern regi...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented a Byte-Per Encoding Tokenizer. However, this python implementation is really slow and so we will use a transformers implementation of Byte-Per Encoding Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import Metaspace, PreTokenizer\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "\n",
    "# setting pre-tokenization to gpt2 tokenizer\n",
    "tokenizer.pre_tokenizer = Metaspace(replacement=\"Ñ\")\n",
    "\n",
    "# Initialize a trainer with desired parameters\n",
    "vocab_size = 30000\n",
    "trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<unk>\", \"<s>\", \"</s>\"])\n",
    "\n",
    "# preprocess data\n",
    "data.text = data.text.apply(text_preprocessing)\n",
    "\n",
    "# Load your training data into a list of strings\n",
    "train_data = data.text.tolist()\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(\n",
    "    train_data,\n",
    "    trainer=trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ñpray', 'ing,', 'Ñshe', \"'s\", 'Ña', 'Ñgood', 'Ñperson']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"praying, she's a good person\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of words\n",
    "data[\"text_length\"] = data.text.apply(lambda x: len(tokenizer.encode(x).tokens))\n",
    "# data = data.query(\"text_length>=5 and text_length<=60\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='text_length', ylabel='Count'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot number of words distribution#mini-gpt\n",
    "\n",
    "sns.histplot(data=data, x=\"text_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset\n",
    "\n",
    "Here we create a training dataset for causal language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18644, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt; anarchism is a political philosophy and mo...</td>\n",
       "      <td>[1, 6530, 774, 710, 2154, 3271, 740, 2780, 807...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;icism toward authority also rose. although ...</td>\n",
       "      <td>[1, 8690, 2677, 4931, 948, 6823, 18, 1764, 112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;s&gt; employs a diversity of tactics in order to...</td>\n",
       "      <td>[1, 19666, 710, 10950, 734, 14346, 736, 1633, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;s&gt; of the prefix an- (\"without\") and the word...</td>\n",
       "      <td>[1, 734, 714, 9418, 727, 17, 2659, 6274, 1035,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;s&gt; first political philosopher to call himsel...</td>\n",
       "      <td>[1, 998, 2154, 2797, 748, 1266, 2461, 727, 730...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;s&gt; a vanguard party, and extreme cultural lib...</td>\n",
       "      <td>[1, 710, 796, 1206, 892, 9581, 740, 4806, 3044...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;s&gt; to the state is central to anarchist thoug...</td>\n",
       "      <td>[1, 748, 714, 1284, 774, 2126, 748, 7302, 1452...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;s&gt;ed as a reaction. the most notable precurso...</td>\n",
       "      <td>[1, 726, 770, 710, 19237, 714, 1054, 4621, 226...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;s&gt; dismissed human law (nomos) and associated...</td>\n",
       "      <td>[1, 12280, 1706, 1513, 2400, 760, 17258, 740, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;s&gt; in private judgment during the reformation...</td>\n",
       "      <td>[1, 736, 3570, 11824, 1222, 714, 14603, 9097, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  <s> anarchism is a political philosophy and mo...   \n",
       "1  <s>icism toward authority also rose. although ...   \n",
       "2  <s> employs a diversity of tactics in order to...   \n",
       "3  <s> of the prefix an- (\"without\") and the word...   \n",
       "4  <s> first political philosopher to call himsel...   \n",
       "5  <s> a vanguard party, and extreme cultural lib...   \n",
       "6  <s> to the state is central to anarchist thoug...   \n",
       "7  <s>ed as a reaction. the most notable precurso...   \n",
       "8  <s> dismissed human law (nomos) and associated...   \n",
       "9  <s> in private judgment during the reformation...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [1, 6530, 774, 710, 2154, 3271, 740, 2780, 807...  \n",
       "1  [1, 8690, 2677, 4931, 948, 6823, 18, 1764, 112...  \n",
       "2  [1, 19666, 710, 10950, 734, 14346, 736, 1633, ...  \n",
       "3  [1, 734, 714, 9418, 727, 17, 2659, 6274, 1035,...  \n",
       "4  [1, 998, 2154, 2797, 748, 1266, 2461, 727, 730...  \n",
       "5  [1, 710, 796, 1206, 892, 9581, 740, 4806, 3044...  \n",
       "6  [1, 748, 714, 1284, 774, 2126, 748, 7302, 1452...  \n",
       "7  [1, 726, 770, 710, 19237, 714, 1054, 4621, 226...  \n",
       "8  [1, 12280, 1706, 1513, 2400, 760, 17258, 740, ...  \n",
       "9  [1, 736, 3570, 11824, 1222, 714, 14603, 9097, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define max sequence length\n",
    "max_seq_len = 128\n",
    "\n",
    "split_tokens_ids = []\n",
    "split_tokens = []\n",
    "\n",
    "# In order to latter add being of sentence token (<s>) and end of sentence token (</s>), we subtract -2 to the max_seq_len\n",
    "seq_len = max_seq_len - 2\n",
    "\n",
    "for id in range(len(data)):\n",
    "    tokens = tokenizer.encode(data.text.tolist()[id]).ids\n",
    "\n",
    "    for i in range(len(tokens) // seq_len):\n",
    "\n",
    "        # split\n",
    "        split_tokens_ids.append(tokenizer.encode(\"<s>\").ids + tokens[i * seq_len : (i + 1) * seq_len] + tokenizer.encode(\"</s>\").ids)\n",
    "\n",
    "        #\n",
    "        split_tokens.append(re.sub(r\"\\s(?!Ñ)\", \"\", tokenizer.decode(split_tokens_ids[i], skip_special_tokens=False)).replace(\"Ñ\", \"\"))\n",
    "\n",
    "\n",
    "# Create a pandas dataframe with the text and tokens ids\n",
    "prepared_data = pd.DataFrame({\"text\": split_tokens, \"tokens\": split_tokens_ids})\n",
    "print(prepared_data.shape)\n",
    "prepared_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Dataset and Data Collator\n",
    "\n",
    "Here we define our custom dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        tokens_ids = self.data.tokens.iloc[idx]\n",
    "        tokens_ids = torch.LongTensor(tokens_ids)\n",
    "\n",
    "        return tokens_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator_for_clm(batch):\n",
    "\n",
    "    tokens_ids = torch.stack(batch)\n",
    "    attention_mask = torch.tril(torch.ones(tokens_ids.shape[0], 1, tokens_ids.shape[1], tokens_ids.shape[1])).bool()\n",
    "\n",
    "    return tokens_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CustomDataset(prepared_data)\n",
    "data_loader = DataLoader(ds, batch_size=2, collate_fn=data_collator_for_clm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids, attention_mask = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 128, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini GPT\n",
    "\n",
    "For this implementation we will implement a architecture similar to the one proposed in the [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) original paper\n",
    "\n",
    "<img src=\"https://i.imgur.com/lgoqvjZ.png\" alt= “” width=\"300px\" height=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DecoderTransformer\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, pf_dim, vocab_size, max_seq_length, n_layers, device=\"cpu\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.decoder = DecoderTransformer(embed_dim, num_heads, dropout, pf_dim, vocab_size, max_seq_length, n_layers, device)\n",
    "        self.output = torch.nn.Linear(embed_dim, vocab_size, bias=False).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        # x shape: (batch_size, max_seq_length)\n",
    "        # mask shape: (batch_size, 1, max_seq_length, max_seq_length)\n",
    "\n",
    "        x = self.decoder(x, mask)  # x shape (batch_size, max_seq_length, embedding_dim)\n",
    "        x = self.output(x)  # x shape: (batch_size, max_seq_length, vocab_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def config_training_args(self, lr=2.5e-4, weight_decay=0.1, scheduler=None, scheduler_kwargs={}, gradient_accumulation=1):\n",
    "\n",
    "        self.gradient_accumulation = gradient_accumulation\n",
    "\n",
    "        grouped_params = self.get_grouped_params(weight_decay=weight_decay)\n",
    "        self.optimizer = torch.optim.AdamW(grouped_params, lr=lr)\n",
    "        # self.optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=0.1)\n",
    "\n",
    "        self.scheduler = scheduler(self.optimizer, **scheduler_kwargs)\n",
    "\n",
    "    def get_grouped_params(self, weight_decay=0.1):\n",
    "\n",
    "        no_decay = [\"bias\", \"layer_norm.weight\", \"embedding\"]\n",
    "        params_with_wd, params_without_wd = [], []\n",
    "\n",
    "        for name, weight in self.named_parameters():\n",
    "\n",
    "            if any([nd in name for nd in no_decay]):\n",
    "                params_without_wd.append(weight)\n",
    "            else:\n",
    "                params_with_wd.append(weight)\n",
    "\n",
    "        return [{\"params\": params_with_wd, \"weight_decay\": weight_decay}, {\"params\": params_without_wd, \"weight_decay\": 0}]\n",
    "\n",
    "    def train_one_epoch(self, train_dataloader):\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        bar = tqdm(train_dataloader, total=len(train_dataloader), leave=True)\n",
    "\n",
    "        # set zero grad\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        for step, (token_ids, attention_mask) in enumerate(bar, 1):\n",
    "\n",
    "            # move to the correct device\n",
    "            token_ids, attention_mask = token_ids.to(self.device), attention_mask.to(self.device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = self(token_ids, attention_mask)\n",
    "\n",
    "            # remove begin of sentence token from labels\n",
    "            labels = token_ids[:, 1:]\n",
    "\n",
    "            # remove end_of_sentence token from outputs\n",
    "            outputs = outputs[:, :-1, :]\n",
    "\n",
    "            # reshape outputs and labels\n",
    "            labels = labels.reshape(-1)\n",
    "            outputs = outputs.reshape(-1, outputs.shape[2])\n",
    "\n",
    "            # compute loss\n",
    "            loss = F.cross_entropy(outputs, labels) / self.gradient_accumulation\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # apply gradient accumulation\n",
    "            if step % self.gradient_accumulation == 0:\n",
    "\n",
    "                # apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
    "\n",
    "                # update weights\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # apply scheduler\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                # set zero grad\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            # running loss\n",
    "            running_loss += loss.item() * self.gradient_accumulation\n",
    "\n",
    "            # print statistics\n",
    "            bar.set_description(f\"Train loss: {running_loss/step:.5f}\")\n",
    "\n",
    "    def train(self, train_dataloader, epochs):\n",
    "\n",
    "        bar = tqdm(range(1, epochs + 1), total=epochs, leave=True)\n",
    "\n",
    "        for epoch in bar:\n",
    "\n",
    "            self.train_one_epoch(train_dataloader)\n",
    "            bar.set_description(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, sentence, tokenizer, temperature):\n",
    "\n",
    "        sentence = \"<s> \" + sentence\n",
    "\n",
    "        tokens_ids = torch.LongTensor(tokenizer.encode(sentence).ids).unsqueeze(0).to(\"cuda\")\n",
    "        attention_mask = torch.ones(tokens_ids.shape[0], 1, tokens_ids.shape[1], tokens_ids.shape[1]).bool().to(\"cuda\")\n",
    "\n",
    "        for _ in range(tokens_ids.shape[-1], self.max_seq_length):\n",
    "\n",
    "            # compute logits\n",
    "            logits = self(tokens_ids, attention_mask)\n",
    "\n",
    "            # scale logits with temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            # computes probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # sample new token using a multinomial distribution\n",
    "            new_token = torch.multinomial(probs, 1)\n",
    "\n",
    "            # update tokens ids with the new predicted token\n",
    "            tokens_ids = torch.cat((tokens_ids, new_token), dim=-1)\n",
    "\n",
    "            # updates the attention mask\n",
    "            attention_mask = torch.ones(tokens_ids.shape[0], 1, tokens_ids.shape[1], tokens_ids.shape[1]).bool().to(\"cuda\")\n",
    "\n",
    "            if new_token == tokenizer.encode(\"</s>\").ids[0]:\n",
    "                break\n",
    "\n",
    "        prediction = tokens_ids.cpu().squeeze(0).tolist()\n",
    "        prediction = re.sub(r\"\\s(?!Ñ)\", \"\", tokenizer.decode(prediction, skip_special_tokens=False)).replace(\"Ñ\", \"\")\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the DecoderTransformer layer\n",
    "embed_dim = 768\n",
    "num_heads = 12\n",
    "dropout = 0.1\n",
    "pf_dim = 3072\n",
    "bs = 32\n",
    "n_layers = 12\n",
    "n_epochs = 5\n",
    "\n",
    "gpt = MiniGPT(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    dropout=dropout,\n",
    "    pf_dim=pf_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_length=max_seq_len,\n",
    "    n_layers=n_layers,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CustomDataset(prepared_data)\n",
    "data_loader = DataLoader(ds, batch_size=bs, collate_fn=data_collator_for_clm, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confing model training args\n",
    "gradient_accumulation = 8\n",
    "lr = 2.5e-4\n",
    "weight_decay = 0.1\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "scheduler_kwargs = {\"T_max\": len(data_loader) * n_epochs // gradient_accumulation, \"eta_min\": 1e-6}\n",
    "\n",
    "\n",
    "gpt.config_training_args(lr, weight_decay, scheduler, scheduler_kwargs, gradient_accumulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432a55b44654469fa2036d3937012e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ee3869bf6c45deb1cf10cdb102475d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2de5d27ed724e8d904ef3ce05ece1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13843ec8f76496489b04e66be83767b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8e367982074550b897552483efcce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6180152e281d409588bffd71dbf6a3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt.train(data_loader, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> anarchism is a political philosophy and platform, james the congress to ised lake flav and isitus, and mut-of its  that medieval until and his he the to is the for hisin the anglican in this and ) – were () the –  he original a the and copper a of and setting of the the of an sodium than cognitive role the them sixth such the as food and (\" of a: with as play ) thea. to and, of appeared on of in these of the northern notes-m . the the average who  of of  aachen a each the and person thisd) (d. ut up and the in of the track</s>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.generate(\"anarchism is a political philosophy and\", tokenizer, temperature=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
