{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Generative Pretrained from Transformers (GPT)\n",
    "\n",
    "This notebooks show a basic implementation of Generate Pretrained from Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# black formatting with jupyter-black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    lab=True,\n",
    "    line_length=140,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Wi will use [wikipedia](https://huggingface.co/datasets/bookcorpus/bookcorpus) as data to pre-train our mini-gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load de dataset\n",
    "data = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[0:500]\", trust_remote_code=True).to_pandas()\n",
    "# data = load_dataset(\"karpathy/tiny_shakespeare\", split=\"train\", trust_remote_code=True).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizer dataset\n",
    "from utils import text_preprocessing\n",
    "\n",
    "data.text = data.text.apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented a Byte-Per Encoding Tokenizer. However, this python implementation is really slow and so we will use a transformers implementation of Byte-Per Encoding Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import Metaspace, PreTokenizer\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "\n",
    "# setting pre-tokenization to gpt2 tokenizer\n",
    "tokenizer.pre_tokenizer = Metaspace(replacement=\"Ñ\")\n",
    "\n",
    "# Initialize a trainer with desired parameters\n",
    "vocab_size = 30000\n",
    "trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<unk>\", \"<s>\", \"</s>\"])\n",
    "\n",
    "# preprocess data\n",
    "data.text = data.text.apply(text_preprocessing)\n",
    "\n",
    "# Load your training data into a list of strings\n",
    "train_data = data.text.tolist()\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(\n",
    "    train_data,\n",
    "    trainer=trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"praying, she's a good person\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of words\n",
    "data[\"text_length\"] = data.text.apply(lambda x: len(tokenizer.encode(x).tokens))\n",
    "# data = data.query(\"text_length>=5 and text_length<=60\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of words distribution\n",
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(data=data, x=\"text_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset\n",
    "\n",
    "Here we create a training dataset for causal language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max sequence length\n",
    "max_seq_len = 128\n",
    "\n",
    "split_tokens_ids = []\n",
    "split_tokens = []\n",
    "\n",
    "# In order to latter add being of sentence token (<s>) and end of sentence token (</s>), we subtract -2 to the max_seq_len\n",
    "seq_len = max_seq_len - 2\n",
    "\n",
    "for id in range(len(data)):\n",
    "    tokens = tokenizer.encode(data.text.tolist()[id]).ids\n",
    "\n",
    "    for i in range(len(tokens) // seq_len):\n",
    "\n",
    "        # split\n",
    "        split_tokens_ids.append(tokenizer.encode(\"<s>\").ids + tokens[i * seq_len : (i + 1) * seq_len] + tokenizer.encode(\"</s>\").ids)\n",
    "\n",
    "        #\n",
    "        split_tokens.append(re.sub(r\"\\s(?!Ñ)\", \"\", tokenizer.decode(split_tokens_ids[i], skip_special_tokens=False)).replace(\"Ñ\", \"\"))\n",
    "\n",
    "\n",
    "# Create a pandas dataframe with the text and tokens ids\n",
    "prepared_data = pd.DataFrame({\"text\": split_tokens, \"tokens\": split_tokens_ids})\n",
    "print(prepared_data.shape)\n",
    "prepared_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Dataset and Data Collator\n",
    "\n",
    "Here we define our custom dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        tokens_ids = self.data.tokens.iloc[idx]\n",
    "        tokens_ids = torch.LongTensor(tokens_ids)\n",
    "\n",
    "        return tokens_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator_for_clm(batch):\n",
    "\n",
    "    tokens_ids = torch.stack(batch)\n",
    "    attention_mask = torch.tril(torch.ones(tokens_ids.shape[0], 1, tokens_ids.shape[1], tokens_ids.shape[1])).bool()\n",
    "\n",
    "    return tokens_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CustomDataset(prepared_data)\n",
    "data_loader = DataLoader(ds, batch_size=2, collate_fn=data_collator_for_clm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids, attention_mask = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini GPT\n",
    "\n",
    "For this implementation we will implement a architecture similar to the one proposed in the [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) original paper\n",
    "\n",
    "<img src=\"https://i.imgur.com/lgoqvjZ.png\" alt= “” width=\"300px\" height=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DecoderTransformer\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, pf_dim, vocab_size, max_seq_length, n_layers, device=\"cpu\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.decoder = DecoderTransformer(embed_dim, num_heads, dropout, pf_dim, vocab_size, max_seq_length, n_layers, device)\n",
    "        self.output = torch.nn.Linear(embed_dim, vocab_size).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        # x shape: (batch_size, max_seq_length)\n",
    "        # mask shape: (batch_size, 1, max_seq_length, max_seq_length)\n",
    "\n",
    "        x = self.decoder(x, mask)  # x shape (batch_size, max_seq_length, embedding_dim)\n",
    "        x = self.output(x)  # x shape: (batch_size, max_seq_length, vocab_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def config_training_args(self, optimizer, optimizer_kwargs={}, scheduler=None, scheduler_kwargs={}):\n",
    "\n",
    "        self.optimizer = optimizer(self.parameters(), **optimizer_kwargs)\n",
    "        self.scheduler = scheduler(self.optimizer, **scheduler_kwargs)\n",
    "\n",
    "    def train_one_epoch(self, train_dataloader):\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        bar = tqdm(train_dataloader, total=len(train_dataloader), leave=True)\n",
    "\n",
    "        for step, (token_ids, attention_mask) in enumerate(bar, 1):\n",
    "\n",
    "            # move to the correct device\n",
    "            token_ids, attention_mask = token_ids.to(self.device), attention_mask.to(self.device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = self(token_ids, attention_mask)\n",
    "\n",
    "            # remove begin of sentence token from labels\n",
    "            labels = token_ids[:, 1:]\n",
    "\n",
    "            # remove end_of_sentence token from outputs\n",
    "            outputs = outputs[:, :-1, :]\n",
    "\n",
    "            # reshape outputs and labels\n",
    "            labels = labels.reshape(-1)\n",
    "            outputs = outputs.reshape(-1, outputs.shape[2])\n",
    "\n",
    "            # compute loss\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "            # set zero grad\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # apply gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
    "\n",
    "            # update weights\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # apply scheduler\n",
    "            self.scheduler.step()\n",
    "\n",
    "            # running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # print statistics\n",
    "            bar.set_description(f\"Train loss: {running_loss/step:.5f}\")\n",
    "\n",
    "    def train(self, train_dataloader, epochs):\n",
    "\n",
    "        bar = tqdm(range(1, epochs + 1), total=epochs, leave=True)\n",
    "\n",
    "        for epoch in bar:\n",
    "\n",
    "            self.train_one_epoch(train_dataloader)\n",
    "            bar.set_description(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    def generate(self, sentence, tokenizer):\n",
    "\n",
    "        sentence = \"<s>\" + sentence\n",
    "\n",
    "        tokens_ids = torch.LongTensor(tokenizer.encode(sentence).ids).unsqueeze(0).to(\"cuda\")\n",
    "        attention_mask = torch.ones(tokens_ids.shape[0], 1, tokens_ids.shape[1], tokens_ids.shape[1]).bool().to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(tokens_ids.shape[-1], self.max_seq_length):\n",
    "\n",
    "                prediction = gpt(tokens_ids, attention_mask)\n",
    "                new_token = prediction[0, -1, :].argmax().item()\n",
    "                tokens_ids = torch.concat((tokens_ids, torch.LongTensor([new_token]).unsqueeze(0).to(\"cuda\")), dim=-1)\n",
    "                attention_mask = torch.ones(tokens_ids.shape[0], 1, tokens_ids.shape[1], tokens_ids.shape[1]).bool().to(\"cuda\")\n",
    "\n",
    "                if new_token == tokenizer.encode(\"</s>\").ids[0]:\n",
    "                    break\n",
    "\n",
    "        prediction = re.sub(r\"\\s(?!Ñ)\", \"\", tokenizer.decode(tokens_ids.cpu().squeeze(0).tolist(), skip_special_tokens=False)).replace(\n",
    "            \"Ñ\", \"\"\n",
    "        )\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the DecoderTransformer layer\n",
    "embed_dim = 768\n",
    "num_heads = 12\n",
    "dropout = 0.1\n",
    "pf_dim = 3072\n",
    "bs = 32\n",
    "n_layers = 12\n",
    "n_epochs = 10\n",
    "\n",
    "gpt = MiniGPT(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    dropout=dropout,\n",
    "    pf_dim=pf_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_length=max_seq_len,\n",
    "    n_layers=n_layers,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CustomDataset(prepared_data)\n",
    "data_loader = DataLoader(ds, batch_size=bs, collate_fn=data_collator_for_clm, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confing model training args\n",
    "optimizer = torch.optim.AdamW\n",
    "optimizer_kwargs = {\"lr\": 2.5e-3, \"weight_decay\": 0.01}\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "scheduler_kwargs = {\"T_max\": len(data_loader) * n_epochs, \"eta_min\": 1e-6}\n",
    "\n",
    "gpt.config_training_args(optimizer, optimizer_kwargs, scheduler, scheduler_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.train(data_loader, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.generate(\"anarchism is a political philosophy and\", tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
